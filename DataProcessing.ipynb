{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/piroo8/Aps360/blob/nicks-stuff/DataProcessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNXbYYuchbAg",
        "outputId": "ec68c360-2b96-4647-a689-8ba365323818"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.7.0.tar.gz (361 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m361.8/361.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.7.0-py2.py3-none-any.whl size=356563 sha256=b55d7c8e1790d4b8cc4d2161293a5cb2fedd7b1f8177717c8e3d308d5e454a3a\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/11/48/5df0b9727d5669c9174a141134f10304d1d78a3b89a4676f3d\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXiCooZc128x",
        "outputId": "b952bf89-e96d-4b1c-dfde-a65a688f4a75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SajpaNh02TMy",
        "outputId": "13444ee0-f4a5-41c8-dfa5-a6f40f3f120f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import emoji\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.sentiment.util import mark_negation\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "import contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKoCatCgg46A",
        "outputId": "e72dd523-c7e4-4536-e4d5-5ce7a9f4795f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lI0NV1o9s1a",
        "outputId": "65f6b154-321f-499c-9b1f-b90f720c5cb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "headers = [\"id\", \"company\", \"sentiment\", \"tweet\"]\n",
        "df_train = pd.read_csv('/content/sample_data/twitter_training.csv', names=headers)\n",
        "df_val = pd.read_csv('/content/sample_data/twitter_validation.csv', names=headers)\n",
        "\n",
        "print(df_train.head())\n",
        "print(df_val.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flolyiWJhe-X",
        "outputId": "855fa2cf-5b3b-449d-f03e-67890652cac3"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     id      company sentiment  \\\n",
            "0  2401  Borderlands  Positive   \n",
            "1  2401  Borderlands  Positive   \n",
            "2  2401  Borderlands  Positive   \n",
            "3  2401  Borderlands  Positive   \n",
            "4  2401  Borderlands  Positive   \n",
            "\n",
            "                                               tweet  \n",
            "0  im getting on borderlands and i will murder yo...  \n",
            "1  I am coming to the borders and I will kill you...  \n",
            "2  im getting on borderlands and i will kill you ...  \n",
            "3  im coming on borderlands and i will murder you...  \n",
            "4  im getting on borderlands 2 and i will murder ...  \n",
            "     id    company   sentiment  \\\n",
            "0  3364   Facebook  Irrelevant   \n",
            "1   352     Amazon     Neutral   \n",
            "2  8312  Microsoft    Negative   \n",
            "3  4371      CS-GO    Negative   \n",
            "4  4433     Google     Neutral   \n",
            "\n",
            "                                               tweet  \n",
            "0  I mentioned on Facebook that I was struggling ...  \n",
            "1  BBC News - Amazon boss Jeff Bezos rejects clai...  \n",
            "2  @Microsoft Why do I pay for WORD when it funct...  \n",
            "3  CSGO matchmaking is so full of closet hacking,...  \n",
            "4  Now the President is slapping Americans in the...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abbreviations = {\n",
        "    \"$\" : \" dollar \",\n",
        "    \"€\" : \" euro \",\n",
        "    \"4ao\" : \"for adults only\",\n",
        "    \"a.m\" : \"before midday\",\n",
        "    \"a3\" : \"anytime anywhere anyplace\",\n",
        "    \"aamof\" : \"as a matter of fact\",\n",
        "    \"acct\" : \"account\",\n",
        "    \"adih\" : \"another day in hell\",\n",
        "    \"afaic\" : \"as far as i am concerned\",\n",
        "    \"afaict\" : \"as far as i can tell\",\n",
        "    \"afaik\" : \"as far as i know\",\n",
        "    \"afair\" : \"as far as i remember\",\n",
        "    \"afk\" : \"away from keyboard\",\n",
        "    \"app\" : \"application\",\n",
        "    \"approx\" : \"approximately\",\n",
        "    \"apps\" : \"applications\",\n",
        "    \"asap\" : \"as soon as possible\",\n",
        "    \"asl\" : \"age, sex, location\",\n",
        "    \"atk\" : \"at the keyboard\",\n",
        "    \"ave.\" : \"avenue\",\n",
        "    \"aymm\" : \"are you my mother\",\n",
        "    \"ayor\" : \"at your own risk\",\n",
        "    \"b&b\" : \"bed and breakfast\",\n",
        "    \"b+b\" : \"bed and breakfast\",\n",
        "    \"b.c\" : \"before christ\",\n",
        "    \"b2b\" : \"business to business\",\n",
        "    \"b2c\" : \"business to customer\",\n",
        "    \"b4\" : \"before\",\n",
        "    \"b4n\" : \"bye for now\",\n",
        "    \"b@u\" : \"back at you\",\n",
        "    \"bae\" : \"before anyone else\",\n",
        "    \"bak\" : \"back at keyboard\",\n",
        "    \"bbbg\" : \"bye bye be good\",\n",
        "    \"bbc\" : \"british broadcasting corporation\",\n",
        "    \"bbias\" : \"be back in a second\",\n",
        "    \"bbl\" : \"be back later\",\n",
        "    \"bbs\" : \"be back soon\",\n",
        "    \"be4\" : \"before\",\n",
        "    \"bfn\" : \"bye for now\",\n",
        "    \"blvd\" : \"boulevard\",\n",
        "    \"bout\" : \"about\",\n",
        "    \"brb\" : \"be right back\",\n",
        "    \"bros\" : \"brothers\",\n",
        "    \"brt\" : \"be right there\",\n",
        "    \"bsaaw\" : \"big smile and a wink\",\n",
        "    \"btw\" : \"by the way\",\n",
        "    \"bwl\" : \"bursting with laughter\",\n",
        "    \"c/o\" : \"care of\",\n",
        "    \"cet\" : \"central european time\",\n",
        "    \"cf\" : \"compare\",\n",
        "    \"cia\" : \"central intelligence agency\",\n",
        "    \"csl\" : \"can not stop laughing\",\n",
        "    \"cu\" : \"see you\",\n",
        "    \"cul8r\" : \"see you later\",\n",
        "    \"cv\" : \"curriculum vitae\",\n",
        "    \"cwot\" : \"complete waste of time\",\n",
        "    \"cya\" : \"see you\",\n",
        "    \"cyt\" : \"see you tomorrow\",\n",
        "    \"dae\" : \"does anyone else\",\n",
        "    \"dbmib\" : \"do not bother me i am busy\",\n",
        "    \"diy\" : \"do it yourself\",\n",
        "    \"dm\" : \"direct message\",\n",
        "    \"dwh\" : \"during work hours\",\n",
        "    \"e123\" : \"easy as one two three\",\n",
        "    \"eet\" : \"eastern european time\",\n",
        "    \"eg\" : \"example\",\n",
        "    \"embm\" : \"early morning business meeting\",\n",
        "    \"encl\" : \"enclosed\",\n",
        "    \"encl.\" : \"enclosed\",\n",
        "    \"etc\" : \"and so on\",\n",
        "    \"faq\" : \"frequently asked questions\",\n",
        "    \"fawc\" : \"for anyone who cares\",\n",
        "    \"fb\" : \"facebook\",\n",
        "    \"fc\" : \"fingers crossed\",\n",
        "    \"fig\" : \"figure\",\n",
        "    \"fimh\" : \"forever in my heart\",\n",
        "    \"ft.\" : \"feet\",\n",
        "    \"ft\" : \"featuring\",\n",
        "    \"ftl\" : \"for the loss\",\n",
        "    \"ftw\" : \"for the win\",\n",
        "    \"fwiw\" : \"for what it is worth\",\n",
        "    \"fyi\" : \"for your information\",\n",
        "    \"g9\" : \"genius\",\n",
        "    \"gahoy\" : \"get a hold of yourself\",\n",
        "    \"gal\" : \"get a life\",\n",
        "    \"gcse\" : \"general certificate of secondary education\",\n",
        "    \"gfn\" : \"gone for now\",\n",
        "    \"gg\" : \"good game\",\n",
        "    \"gl\" : \"good luck\",\n",
        "    \"glhf\" : \"good luck have fun\",\n",
        "    \"gmt\" : \"greenwich mean time\",\n",
        "    \"gmta\" : \"great minds think alike\",\n",
        "    \"gn\" : \"good night\",\n",
        "    \"g.o.a.t\" : \"greatest of all time\",\n",
        "    \"goat\" : \"greatest of all time\",\n",
        "    \"goi\" : \"get over it\",\n",
        "    \"gps\" : \"global positioning system\",\n",
        "    \"gr8\" : \"great\",\n",
        "    \"gratz\" : \"congratulations\",\n",
        "    \"gyal\" : \"girl\",\n",
        "    \"h&c\" : \"hot and cold\",\n",
        "    \"hp\" : \"horsepower\",\n",
        "    \"hr\" : \"hour\",\n",
        "    \"hrh\" : \"his royal highness\",\n",
        "    \"ht\" : \"height\",\n",
        "    \"ibrb\" : \"i will be right back\",\n",
        "    \"ic\" : \"i see\",\n",
        "    \"icq\" : \"i seek you\",\n",
        "    \"icymi\" : \"in case you missed it\",\n",
        "    \"idc\" : \"i do not care\",\n",
        "    \"idgadf\" : \"i do not give a damn fuck\",\n",
        "    \"idgaf\" : \"i do not give a fuck\",\n",
        "    \"idk\" : \"i do not know\",\n",
        "    \"ie\" : \"that is\",\n",
        "    \"i.e\" : \"that is\",\n",
        "    \"ifyp\" : \"i feel your pain\",\n",
        "    \"IG\" : \"instagram\",\n",
        "    \"iirc\" : \"if i remember correctly\",\n",
        "    \"ilu\" : \"i love you\",\n",
        "    \"ily\" : \"i love you\",\n",
        "    \"imho\" : \"in my humble opinion\",\n",
        "    \"imo\" : \"in my opinion\",\n",
        "    \"imu\" : \"i miss you\",\n",
        "    \"iow\" : \"in other words\",\n",
        "    \"irl\" : \"in real life\",\n",
        "    \"j4f\" : \"just for fun\",\n",
        "    \"jic\" : \"just in case\",\n",
        "    \"jk\" : \"just kidding\",\n",
        "    \"jsyk\" : \"just so you know\",\n",
        "    \"l8r\" : \"later\",\n",
        "    \"lb\" : \"pound\",\n",
        "    \"lbs\" : \"pounds\",\n",
        "    \"ldr\" : \"long distance relationship\",\n",
        "    \"lmao\" : \"laugh my ass off\",\n",
        "    \"lmfao\" : \"laugh my fucking ass off\",\n",
        "    \"lol\" : \"laughing out loud\",\n",
        "    \"ltd\" : \"limited\",\n",
        "    \"ltns\" : \"long time no see\",\n",
        "    \"m8\" : \"mate\",\n",
        "    \"mf\" : \"motherfucker\",\n",
        "    \"mfs\" : \"motherfuckers\",\n",
        "    \"mfw\" : \"my face when\",\n",
        "    \"mofo\" : \"motherfucker\",\n",
        "    \"mph\" : \"miles per hour\",\n",
        "    \"mr\" : \"mister\",\n",
        "    \"mrw\" : \"my reaction when\",\n",
        "    \"ms\" : \"miss\",\n",
        "    \"mte\" : \"my thoughts exactly\",\n",
        "    \"nagi\" : \"not a good idea\",\n",
        "    \"nbc\" : \"national broadcasting company\",\n",
        "    \"nbd\" : \"not big deal\",\n",
        "    \"nfs\" : \"not for sale\",\n",
        "    \"ngl\" : \"not going to lie\",\n",
        "    \"nhs\" : \"national health service\",\n",
        "    \"nrn\" : \"no reply necessary\",\n",
        "    \"nsfl\" : \"not safe for life\",\n",
        "    \"nsfw\" : \"not safe for work\",\n",
        "    \"nth\" : \"nice to have\",\n",
        "    \"nvr\" : \"never\",\n",
        "    \"nyc\" : \"new york city\",\n",
        "    \"oc\" : \"original content\",\n",
        "    \"og\" : \"original\",\n",
        "    \"ohp\" : \"overhead projector\",\n",
        "    \"oic\" : \"oh i see\",\n",
        "    \"omdb\" : \"over my dead body\",\n",
        "    \"omg\" : \"oh my god\",\n",
        "    \"omw\" : \"on my way\",\n",
        "    \"p.a\" : \"per annum\",\n",
        "    \"p.m\" : \"after midday\",\n",
        "    \"pm\" : \"prime minister\",\n",
        "    \"poc\" : \"people of color\",\n",
        "    \"pov\" : \"point of view\",\n",
        "    \"pp\" : \"pages\",\n",
        "    \"ppl\" : \"people\",\n",
        "    \"prw\" : \"parents are watching\",\n",
        "    \"ps\" : \"postscript\",\n",
        "    \"pt\" : \"point\",\n",
        "    \"ptb\" : \"please text back\",\n",
        "    \"pto\" : \"please turn over\",\n",
        "    \"qpsa\" : \"what happens\", #\"que pasa\",\n",
        "    \"ratchet\" : \"rude\",\n",
        "    \"rbtl\" : \"read between the lines\",\n",
        "    \"rlrt\" : \"real life retweet\",\n",
        "    \"rofl\" : \"rolling on the floor laughing\",\n",
        "    \"roflol\" : \"rolling on the floor laughing out loud\",\n",
        "    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
        "    \"rt\" : \"retweet\",\n",
        "    \"ruok\" : \"are you ok\",\n",
        "    \"sfw\" : \"safe for work\",\n",
        "    \"sk8\" : \"skate\",\n",
        "    \"smh\" : \"shake my head\",\n",
        "    \"sq\" : \"square\",\n",
        "    \"srsly\" : \"seriously\",\n",
        "    \"ssdd\" : \"same stuff different day\",\n",
        "    \"tbh\" : \"to be honest\",\n",
        "    \"tbs\" : \"tablespooful\",\n",
        "    \"tbsp\" : \"tablespooful\",\n",
        "    \"tfw\" : \"that feeling when\",\n",
        "    \"thks\" : \"thank you\",\n",
        "    \"tho\" : \"though\",\n",
        "    \"thx\" : \"thank you\",\n",
        "    \"tia\" : \"thanks in advance\",\n",
        "    \"til\" : \"today i learned\",\n",
        "    \"tl;dr\" : \"too long i did not read\",\n",
        "    \"tldr\" : \"too long i did not read\",\n",
        "    \"tmb\" : \"tweet me back\",\n",
        "    \"tntl\" : \"trying not to laugh\",\n",
        "    \"ttyl\" : \"talk to you later\",\n",
        "    \"u\" : \"you\",\n",
        "    \"u2\" : \"you too\",\n",
        "    \"u4e\" : \"yours for ever\",\n",
        "    \"utc\" : \"coordinated universal time\",\n",
        "    \"w/\" : \"with\",\n",
        "    \"w/o\" : \"without\",\n",
        "    \"w8\" : \"wait\",\n",
        "    \"wassup\" : \"what is up\",\n",
        "    \"wb\" : \"welcome back\",\n",
        "    \"wtf\" : \"what the fuck\",\n",
        "    \"wtg\" : \"way to go\",\n",
        "    \"wtpa\" : \"where the party at\",\n",
        "    \"wuf\" : \"where are you from\",\n",
        "    \"wuzup\" : \"what is up\",\n",
        "    \"wywh\" : \"wish you were here\",\n",
        "    \"yd\" : \"yard\",\n",
        "    \"ygtr\" : \"you got that right\",\n",
        "    \"ynk\" : \"you never know\",\n",
        "    \"zzz\" : \"sleeping bored and tired\"\n",
        "}"
      ],
      "metadata": {
        "id": "m9Qu96p33uTa"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Remove NaN values\n",
        "def remove_nan_values(df, columns):\n",
        "    for column in columns:\n",
        "        df = df[df[column].notna()]  # Remove rows with NaN values\n",
        "        df = df[df[column].astype(bool)]  # Remove rows with empty lists or lists with only white spaces\n",
        "    return df\n",
        "\n",
        "# Step 2: Assign tweet labels\n",
        "def one_hot_encode_sentiments(sentiment):\n",
        "    if sentiment == 'Positive':\n",
        "        return 0\n",
        "    elif sentiment == 'Negative':\n",
        "        return 1\n",
        "    elif sentiment == 'Neutral':\n",
        "        return 2\n",
        "    elif sentiment == 'Irrelevant':\n",
        "        return 3\n",
        "    else:\n",
        "      return '?><?'\n",
        "\n",
        "def expand_slang(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [abbreviations.get(word.lower(), word) for word in tokens]\n",
        "    text = ' '.join(tokens)\n",
        "    return text\n",
        "\n",
        "\n",
        "# Step 3: Clean tweets\n",
        "def clean_tweets(tweet):\n",
        "    # Remove @mentions\n",
        "    tweet = re.sub(r'@\\w+', '', tweet)\n",
        "\n",
        "    # Remove URLs\n",
        "    tweet = re.sub(r'http\\S+|www\\S+', '', tweet)\n",
        "\n",
        "    # Remove hashtags\n",
        "    tweet = re.sub(r'#\\w+', '', tweet)\n",
        "\n",
        "    # Convert emojis to text\n",
        "    tweet = emoji.demojize(tweet)\n",
        "\n",
        "    # Expand contractions\n",
        "    tweet = contractions.fix(tweet)\n",
        "    ##############################3\n",
        "\n",
        "    # Remove numbers\n",
        "    tweet = re.sub(r'\\d+', '', tweet)\n",
        "\n",
        "    # Remove punctuation\n",
        "    tweet = re.sub(r'[^\\w\\s]', '', tweet)\n",
        "\n",
        "    # Remove excessive whitespace\n",
        "    tweet = re.sub(r'\\s+', ' ', tweet)\n",
        "\n",
        "    # Standardize repeated characters\n",
        "    tweet = re.sub(r'(.)\\1+', r'\\1\\1', tweet)\n",
        "\n",
        "    #Expand slang terms\n",
        "    tweet = expand_slang(tweet)\n",
        "\n",
        "    # Tokenize the tweet\n",
        "    tokens = word_tokenize(tweet)\n",
        "\n",
        "    # Lowercase and normalize\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "\n",
        "    # Handle abbreviations and acronyms\n",
        "    abbreviation_words = {\"lol\": \"laughing out loud\", \"btw\": \"by the way\"}  # Add more as needed\n",
        "    tokens = [abbreviation_words.get(token.lower(), token) for token in tokens]\n",
        "\n",
        "    # Handle negation using mark_negation\n",
        "    #negation_tokens = mark_negation(tokens)\n",
        "    #tweet = \"I don't like this product. It is not good.\"\n",
        "    #['I', \"don't\", 'like_NEG', 'this_NEG', 'product_NEG', '.', 'It_NEG', 'is_NEG', 'not_NEG', 'good_NEG', '.']\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    # Uncomment the following lines to include POS tags as comments\n",
        "    #pos_tags = nltk.pos_tag(tokens)\n",
        "    #tokens_with_pos = [f\"{token}/{pos}\" for token, pos in pos_tags]\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "i-k7hoKaf1Cp"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Remove NaN values\n",
        "columns_to_check = [\"id\", \"company\", \"sentiment\", \"tweet\"]\n",
        "df_train = remove_nan_values(df_train, columns_to_check)\n",
        "df_val = remove_nan_values(df_val, columns_to_check)\n",
        "\n",
        "print(df_train.head())\n",
        "print(df_val.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0gfIa_NjhVI",
        "outputId": "df5e9ab7-a8f6-4754-cc43-8daea8f8ddc6"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     id      company sentiment  \\\n",
            "0  2401  Borderlands  Positive   \n",
            "1  2401  Borderlands  Positive   \n",
            "2  2401  Borderlands  Positive   \n",
            "3  2401  Borderlands  Positive   \n",
            "4  2401  Borderlands  Positive   \n",
            "\n",
            "                                               tweet  \n",
            "0  im getting on borderlands and i will murder yo...  \n",
            "1  I am coming to the borders and I will kill you...  \n",
            "2  im getting on borderlands and i will kill you ...  \n",
            "3  im coming on borderlands and i will murder you...  \n",
            "4  im getting on borderlands 2 and i will murder ...  \n",
            "     id    company   sentiment  \\\n",
            "0  3364   Facebook  Irrelevant   \n",
            "1   352     Amazon     Neutral   \n",
            "2  8312  Microsoft    Negative   \n",
            "3  4371      CS-GO    Negative   \n",
            "4  4433     Google     Neutral   \n",
            "\n",
            "                                               tweet  \n",
            "0  I mentioned on Facebook that I was struggling ...  \n",
            "1  BBC News - Amazon boss Jeff Bezos rejects clai...  \n",
            "2  @Microsoft Why do I pay for WORD when it funct...  \n",
            "3  CSGO matchmaking is so full of closet hacking,...  \n",
            "4  Now the President is slapping Americans in the...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Assign tweet labels\n",
        "df_train['sentiment'] = df_train['sentiment'].apply(one_hot_encode_sentiments)\n",
        "df_val['sentiment'] = df_val['sentiment'].apply(one_hot_encode_sentiments)\n",
        "\n",
        "print(df_train.head())\n",
        "print(df_val.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMG25EKJjjDf",
        "outputId": "9583684a-7e25-4d13-aa1d-f6765ea449d3"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     id      company  sentiment  \\\n",
            "0  2401  Borderlands          0   \n",
            "1  2401  Borderlands          0   \n",
            "2  2401  Borderlands          0   \n",
            "3  2401  Borderlands          0   \n",
            "4  2401  Borderlands          0   \n",
            "\n",
            "                                               tweet  \n",
            "0  im getting on borderlands and i will murder yo...  \n",
            "1  I am coming to the borders and I will kill you...  \n",
            "2  im getting on borderlands and i will kill you ...  \n",
            "3  im coming on borderlands and i will murder you...  \n",
            "4  im getting on borderlands 2 and i will murder ...  \n",
            "     id    company  sentiment  \\\n",
            "0  3364   Facebook          3   \n",
            "1   352     Amazon          2   \n",
            "2  8312  Microsoft          1   \n",
            "3  4371      CS-GO          1   \n",
            "4  4433     Google          2   \n",
            "\n",
            "                                               tweet  \n",
            "0  I mentioned on Facebook that I was struggling ...  \n",
            "1  BBC News - Amazon boss Jeff Bezos rejects clai...  \n",
            "2  @Microsoft Why do I pay for WORD when it funct...  \n",
            "3  CSGO matchmaking is so full of closet hacking,...  \n",
            "4  Now the President is slapping Americans in the...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Clean tweets\n",
        "df_train['tweet'] = df_train['tweet'].apply(clean_tweets)\n",
        "df_val['tweet'] = df_val['tweet'].apply(clean_tweets)\n",
        "\n",
        "print(df_train.head())\n",
        "print(df_val.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_31I56nRjkKH",
        "outputId": "cb14c669-abfb-48f5-edab-e38d9b38d0d4"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     id      company  sentiment                          tweet\n",
            "0  2401  Borderlands          0  [getting, borderland, murder]\n",
            "1  2401  Borderlands          0         [coming, border, kill]\n",
            "2  2401  Borderlands          0    [getting, borderland, kill]\n",
            "3  2401  Borderlands          0   [coming, borderland, murder]\n",
            "4  2401  Borderlands          0  [getting, borderland, murder]\n",
            "     id    company  sentiment  \\\n",
            "0  3364   Facebook          3   \n",
            "1   352     Amazon          2   \n",
            "2  8312  Microsoft          1   \n",
            "3  4371      CS-GO          1   \n",
            "4  4433     Google          2   \n",
            "\n",
            "                                               tweet  \n",
            "0  [mentioned, facebook, struggling, motivation, ...  \n",
            "1  [british, broadcasting, corporation, news, ama...  \n",
            "2  [pay, word, function, poorly, chromebook, face...  \n",
            "3  [csgo, matchmaking, full, closet, hacking, tru...  \n",
            "4  [president, slapping, american, face, really, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# Assuming you have a DataFrame named df_train with columns 'sentiment' and 'tweet'\n",
        "\n",
        "# Step 1: Collect the tweets and sentiment encodings from the DataFrame\n",
        "tweets = df_train['tweet'].tolist()\n",
        "sentiments = df_train['sentiment'].tolist()\n",
        "\n",
        "# Step 3: Combine the tweets with their corresponding sentiment encodings into a list of tuples\n",
        "tweet_sentiment_pairs = list(zip(tweets, sentiments))\n",
        "\n",
        "# Step 4: Count the frequency of each word for each sentiment\n",
        "word_frequencies = {\n",
        "    tuple([1, 0, 0, 0]): Counter(),\n",
        "    tuple([0, 1, 0, 0]): Counter(),\n",
        "    tuple([0, 0, 1, 0]): Counter(),\n",
        "    tuple([0, 0, 0, 1]): Counter()\n",
        "}\n",
        "\n",
        "for tweet, sentiment in tweet_sentiment_pairs:\n",
        "    word_frequencies[tuple(sentiment)].update(tweet)\n",
        "\n",
        "# Step 5: Determine the most frequent words for each sentiment\n",
        "most_frequent_words = {\n",
        "    tuple([1, 0, 0, 0]): [],\n",
        "    tuple([0, 1, 0, 0]): [],\n",
        "    tuple([0, 0, 1, 0]): [],\n",
        "    tuple([0, 0, 0, 1]): []\n",
        "}\n",
        "infrequent_words = set()\n",
        "\n",
        "for sentiment, counter in word_frequencies.items():\n",
        "    most_common_words = counter.most_common()\n",
        "    most_frequent_words[sentiment] = most_common_words\n",
        "\n",
        "    # Step 6: Handle infrequent words\n",
        "    frequency_threshold = 5  # Adjust this threshold as needed\n",
        "    infrequent_words.update(word for word, frequency in most_common_words if frequency <= frequency_threshold)\n",
        "\n",
        "# Print the most frequent words for each sentiment\n",
        "sentiment_mapping = {\n",
        "    tuple([1, 0, 0, 0]): 'Positive',\n",
        "    tuple([0, 1, 0, 0]): 'Negative',\n",
        "    tuple([0, 0, 1, 0]): 'Neutral',\n",
        "    tuple([0, 0, 0, 1]): 'Irrelevant'\n",
        "}\n",
        "\n",
        "for sentiment, words in most_frequent_words.items():\n",
        "    print(f\"Most frequent words for sentiment {sentiment_mapping[sentiment]}:\")\n",
        "    for word, frequency in words:\n",
        "        print(f\"{word}: {frequency}\")\n",
        "    print()\n",
        "\n",
        "# Example of handling infrequent words\n",
        "print(\"Infrequent words:\")\n",
        "print(infrequent_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "qbA0cO2xAr1K",
        "outputId": "7d03ed94-a483-429d-805c-e043d5b207cb"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-142-6785ee2520cf>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweet_sentiment_pairs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mword_frequencies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Step 5: Determine the most frequent words for each sentiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Remove NaN values\n",
        "columns_to_check = [\"id\", \"company\", \"sentiment\", \"tweet\"]\n",
        "df_train = remove_nan_values(df_train, columns_to_check)\n",
        "df_val = remove_nan_values(df_val, columns_to_check)\n",
        "\n",
        "print(df_train.head())\n",
        "print(df_val.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_-TpSONE4qr",
        "outputId": "b404f505-922d-4b89-bfae-08cc787c5115"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      id      company  sentiment  \\\n",
            "12  2403  Borderlands          2   \n",
            "13  2403  Borderlands          2   \n",
            "14  2403  Borderlands          2   \n",
            "15  2403  Borderlands          2   \n",
            "16  2403  Borderlands          2   \n",
            "\n",
            "                                                tweet  \n",
            "12  [rockhard, la, varlope, rare, powerful, handso...  \n",
            "13  [rockhard, la, varlope, rare, powerful, handso...  \n",
            "14  [rockhard, la, varlope, rare, powerful, handso...  \n",
            "15  [rockhard, la, vita, rare, powerful, handsome,...  \n",
            "16  [live, rock, hard, music, la, la, varlope, rar...  \n",
            "     id    company  sentiment  \\\n",
            "0  3364   Facebook          3   \n",
            "1   352     Amazon          2   \n",
            "2  8312  Microsoft          1   \n",
            "3  4371      CS-GO          1   \n",
            "4  4433     Google          2   \n",
            "\n",
            "                                               tweet  \n",
            "0  [mentioned, facebook, struggling, motivation, ...  \n",
            "1  [british, broadcasting, corporation, news, ama...  \n",
            "2  [pay, word, function, poorly, chromebook, face...  \n",
            "3  [csgo, matchmaking, full, closet, hacking, tru...  \n",
            "4  [president, slapping, american, face, really, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the resulting DataFrame\n",
        "print(df_train.head())\n",
        "print(df_val.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqrFes9ajlOo",
        "outputId": "d20776ce-feb5-42c9-d88f-b6106efc0c17"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      id      company  sentiment  \\\n",
            "12  2403  Borderlands          2   \n",
            "13  2403  Borderlands          2   \n",
            "14  2403  Borderlands          2   \n",
            "15  2403  Borderlands          2   \n",
            "16  2403  Borderlands          2   \n",
            "\n",
            "                                                tweet  \n",
            "12  [rockhard, la, varlope, rare, powerful, handso...  \n",
            "13  [rockhard, la, varlope, rare, powerful, handso...  \n",
            "14  [rockhard, la, varlope, rare, powerful, handso...  \n",
            "15  [rockhard, la, vita, rare, powerful, handsome,...  \n",
            "16  [live, rock, hard, music, la, la, varlope, rar...  \n",
            "     id    company  sentiment  \\\n",
            "0  3364   Facebook          3   \n",
            "1   352     Amazon          2   \n",
            "2  8312  Microsoft          1   \n",
            "3  4371      CS-GO          1   \n",
            "4  4433     Google          2   \n",
            "\n",
            "                                               tweet  \n",
            "0  [mentioned, facebook, struggling, motivation, ...  \n",
            "1  [british, broadcasting, corporation, news, ama...  \n",
            "2  [pay, word, function, poorly, chromebook, face...  \n",
            "3  [csgo, matchmaking, full, closet, hacking, tru...  \n",
            "4  [president, slapping, american, face, really, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming your DataFrame is named df\n",
        "import torch\n",
        "# Drop the 'id' column\n",
        "df_train_new = df_train.drop('id', axis=1)\n",
        "df_val_new = df_val.drop('id', axis=1)\n",
        "\n",
        "# Export the DataFrame to a CSV file\n",
        "df_train_new.to_csv('df_train_new_int.csv', index=False)\n",
        "df_val_new.to_csv('df_val_new_int.csv', index=False)\n",
        "\n",
        "#create dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(df_train_new, batch_size=64, shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(df_val_new, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "DR0xoId7Du2U"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_train_new[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yo6v0HiINkoD",
        "outputId": "efa0d238-b16c-43a9-f06e-470096f25ff2"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        company  sentiment                                              tweet\n",
            "12  Borderlands          2  [rockhard, la, varlope, rare, powerful, handso...\n",
            "13  Borderlands          2  [rockhard, la, varlope, rare, powerful, handso...\n",
            "14  Borderlands          2  [rockhard, la, varlope, rare, powerful, handso...\n",
            "15  Borderlands          2  [rockhard, la, vita, rare, powerful, handsome,...\n",
            "16  Borderlands          2  [live, rock, hard, music, la, la, varlope, rar...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from collections import Counter\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "# Assuming you have a series of lists in your DataFrame column 'tweet'\n",
        "# For example, assuming your DataFrame is named 'df_train_new' and 'df_val_new', and the column with lists is 'tweet'\n",
        "series_of_lists_train = df_train_new['tweet']\n",
        "series_of_lists_valid = df_val_new['tweet']\n",
        "\n",
        "# Step 1: Flatten the series of lists into a single list for both training and validation sets\n",
        "all_words = [word for sublist in series_of_lists_train for word in sublist]\n",
        "all_words_valid = [word for sublist in series_of_lists_valid for word in sublist]\n",
        "\n",
        "# Combine the words from both training and validation sets\n",
        "all_words.extend(all_words_valid)\n",
        "\n",
        "# Step 2: Count the occurrences of each word\n",
        "word_counts = Counter(all_words)\n",
        "\n",
        "# Step 3: Sort the words based on their frequency in descending order\n",
        "sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Step 4: Create a vocabulary dictionary with words ordered by frequency\n",
        "vocab_dict = {word: idx for idx, (word, _) in enumerate(sorted_words)}\n",
        "\n",
        "# Step 5: Convert the series of lists into sequences of indices using the vocabulary dictionary\n",
        "sequences_of_indices_train = [[vocab_dict[word] for word in sublist] for sublist in series_of_lists_train]\n",
        "sequences_of_indices_valid = [[vocab_dict[word] for word in sublist] for sublist in series_of_lists_valid]\n",
        "\n",
        "# Step 6: Pad the sequences with zeros to make them all the same length\n",
        "# Find the maximum length of a sequence in both training and validation sets combined\n",
        "max_seq_length_train = max(len(seq) for seq in sequences_of_indices_train)\n",
        "max_seq_length_valid = max(len(seq) for seq in sequences_of_indices_valid)\n",
        "max_seq_length = max(max_seq_length_train, max_seq_length_valid)\n",
        "\n",
        "padded_sequences_train = pad_sequence([torch.tensor(seq[:max_seq_length]) for seq in sequences_of_indices_train], batch_first=True, padding_value=0)\n",
        "padded_sequences_valid = pad_sequence([torch.tensor(seq + [0] * (max_seq_length - len(seq))) for seq in sequences_of_indices_valid], batch_first=True, padding_value=0)\n",
        "\n",
        "\n",
        "# Ensure the shapes of both padded sequences\n",
        "print(padded_sequences_train.shape)\n",
        "print(padded_sequences_valid.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFg6xWObTFxQ",
        "outputId": "ce27fcbe-b13d-42b6-daf5-b504bed607bb"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([52141, 198])\n",
            "torch.Size([722, 198])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_labels_list = torch.tensor(df_train_new['sentiment'])\n",
        "# val_labels_list = torch.tensor(df_val_new['sentiment'])\n",
        "train_label_list = []\n",
        "valid_label_list = []\n",
        "for label in df_train_new['sentiment']:\n",
        "  train_label_list.append(label)\n",
        "for label in df_val_new['sentiment']:\n",
        "  valid_label_list.append(label)\n",
        "train_labels_tensor = torch.tensor(train_label_list)\n",
        "valid_labels_tensor = torch.tensor(valid_label_list)"
      ],
      "metadata": {
        "id": "AFQNRjT8Rj48"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, sequences, labels, h0_c0=None):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "        self.h0_c0 = h0_c0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence = self.sequences[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.h0_c0 is not None:\n",
        "            h0_c0 = self.h0_c0[idx]\n",
        "            return sequence, label, h0_c0\n",
        "        else:\n",
        "            return sequence, label, None\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Separate the elements of the batch into individual lists\n",
        "    sequences, labels, h0_c0 = zip(*batch)\n",
        "\n",
        "    # Pad the sequences with zeros to make them the same length\n",
        "    padded_sequences = pad_sequence(sequences, batch_first=True)\n",
        "\n",
        "    # Stack the tensors\n",
        "    labels = torch.tensor(labels)\n",
        "\n",
        "    # Filter out None values in h0_c0\n",
        "    h0_c0 = [tensor for tensor in h0_c0 if tensor is not None]\n",
        "\n",
        "    if h0_c0:\n",
        "        # Separate h0 and c0 tensors\n",
        "        h0 = torch.stack([tensor[0] for tensor in h0_c0])\n",
        "        c0 = torch.stack([tensor[1] for tensor in h0_c0])\n",
        "    else:\n",
        "        # If h0_c0 is empty, create tensors of zeros with appropriate shapes\n",
        "        batch_size = padded_sequences.size(0)\n",
        "        num_layers = 1  # Set the appropriate num_layers\n",
        "        hidden_size = 128  # Set the appropriate hidden_size\n",
        "\n",
        "        h0 = torch.zeros(num_layers, batch_size, hidden_size).to(padded_sequences.device)\n",
        "        c0 = torch.zeros(num_layers, batch_size, hidden_size).to(padded_sequences.device)\n",
        "\n",
        "    return padded_sequences, labels, h0, c0\n",
        "# ... (your data preprocessing steps) ...\n",
        "\n",
        "# Convert sequences and labels to torch.Tensor\n",
        "sequences_tensor = [torch.tensor(seq) for seq in sequences_of_indices_train]\n",
        "labels_tensor = torch.tensor(train_label_list)\n",
        "\n",
        "# Create custom dataset and data loader with collate_fn\n",
        "train_dataset = CustomDataset(sequences_tensor, labels_tensor)\n",
        "batch_size = 32\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "z8L2aVi6Ef-p"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepared_train_dataset = torch.utils.data.TensorDataset(padded_sequences_train, train_labels_tensor)\n",
        "prepared_valid_dataset = torch.utils.data.TensorDataset(padded_sequences_valid, valid_labels_tensor)\n",
        "\n",
        "train_data_loader = torch.utils.data.DataLoader(prepared_train_dataset, batch_size=32, shuffle=True)\n",
        "valid_data_loader = torch.utils.data.DataLoader(prepared_valid_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "9Ts6EpUo4CND"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_data, batch_labels in valid_data_loader:\n",
        "    print(\"Data:\", batch_data)\n",
        "    print(\"Labels:\", batch_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1TGygpV5BAL",
        "outputId": "b6a2e3b2-cdc9-4b81-a5a4-c6afc576b0b0"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data: tensor([[   51,    70,    10,  ...,     0,     0,     0],\n",
            "        [  427,  1793,   302,  ...,     0,     0,     0],\n",
            "        [  177,     2,    79,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [ 1660,     6, 10452,  ...,     0,     0,     0],\n",
            "        [29374,  1296,   174,  ...,     0,     0,     0],\n",
            "        [10113, 10114,  1311,  ...,     0,     0,     0]])\n",
            "Labels: tensor([3, 1, 2, 2, 3, 3, 3, 2, 3, 1, 1, 1, 1, 1, 2, 2, 3, 3, 1, 1, 1, 1, 1, 2,\n",
            "        1, 2, 3, 2, 3, 3, 2, 3])\n",
            "Data: tensor([[  91,  180,   20,  ...,    0,    0,    0],\n",
            "        [1011, 7326,   47,  ...,    0,    0,    0],\n",
            "        [ 283,   18, 1344,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [2671,  829,    0,  ...,    0,    0,    0],\n",
            "        [ 286,  125, 3615,  ...,    0,    0,    0],\n",
            "        [  11,    2,  542,  ...,    0,    0,    0]])\n",
            "Labels: tensor([1, 2, 3, 3, 2, 1, 1, 2, 3, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 2, 1, 2, 1, 2,\n",
            "        2, 1, 1, 2, 2, 3, 1, 3])\n",
            "Data: tensor([[2865,   84,  666,  ...,    0,    0,    0],\n",
            "        [ 150,  162, 6558,  ...,    0,    0,    0],\n",
            "        [ 326, 1238,    0,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [3773, 3156,    4,  ...,    0,    0,    0],\n",
            "        [ 243,  559,  257,  ...,    0,    0,    0],\n",
            "        [ 297,   40,    1,  ...,    0,    0,    0]])\n",
            "Labels: tensor([2, 2, 3, 1, 1, 1, 2, 2, 3, 3, 3, 3, 1, 1, 2, 2, 1, 1, 1, 3, 1, 2, 3, 2,\n",
            "        1, 2, 1, 1, 2, 2, 2, 1])\n",
            "Data: tensor([[ 387, 7694,  865,  ...,    0,    0,    0],\n",
            "        [ 150,    9,   17,  ...,    0,    0,    0],\n",
            "        [  97, 2173,   57,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 269, 1710, 3268,  ...,    0,    0,    0],\n",
            "        [  36,   21, 2761,  ...,    0,    0,    0],\n",
            "        [6754,    5,    4,  ...,    0,    0,    0]])\n",
            "Labels: tensor([2, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 1, 2, 2, 2, 1, 1, 2, 1, 2, 1, 2, 1,\n",
            "        3, 3, 1, 3, 1, 2, 3, 1])\n",
            "Data: tensor([[ 524, 3492,   35,  ...,    0,    0,    0],\n",
            "        [ 112, 1365, 5960,  ...,    0,    0,    0],\n",
            "        [  36,    3,   37,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [  91,  174,   97,  ...,    0,    0,    0],\n",
            "        [ 822,  590,  750,  ...,    0,    0,    0],\n",
            "        [4334, 2443,  481,  ...,    0,    0,    0]])\n",
            "Labels: tensor([1, 2, 3, 1, 3, 1, 3, 2, 2, 2, 1, 1, 3, 2, 2, 2, 2, 2, 1, 2, 3, 3, 2, 3,\n",
            "        1, 2, 1, 1, 3, 3, 3, 2])\n",
            "Data: tensor([[   64,   137,    37,  ...,     0,     0,     0],\n",
            "        [   51, 12463,    10,  ...,     0,     0,     0],\n",
            "        [  232,   148,    73,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [ 1119,    65,  6257,  ...,     0,     0,     0],\n",
            "        [    8,   160,    32,  ...,     0,     0,     0],\n",
            "        [  407,  2726,  1099,  ...,     0,     0,     0]])\n",
            "Labels: tensor([2, 1, 3, 2, 1, 3, 1, 2, 3, 1, 1, 2, 1, 3, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2,\n",
            "        2, 2, 1, 2, 1, 2, 2, 2])\n",
            "Data: tensor([[  138,   251,  1255,  ...,     0,     0,     0],\n",
            "        [  524,   130,     1,  ...,     0,     0,     0],\n",
            "        [ 1545,   576,    90,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [10204,   369, 10205,  ...,     0,     0,     0],\n",
            "        [  368,  2133,  2370,  ...,     0,     0,     0],\n",
            "        [  447,    77,   604,  ...,     0,     0,     0]])\n",
            "Labels: tensor([2, 1, 1, 1, 3, 3, 1, 3, 2, 1, 2, 2, 3, 2, 1, 2, 2, 2, 1, 1, 3, 1, 3, 2,\n",
            "        1, 2, 2, 3, 1, 1, 2, 3])\n",
            "Data: tensor([[ 150,  162,   62,  ...,    0,    0,    0],\n",
            "        [  51,   70,   10,  ...,    0,    0,    0],\n",
            "        [ 141,  342,  718,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 318,  737,   70,  ...,    0,    0,    0],\n",
            "        [1081,   42,    0,  ...,    0,    0,    0],\n",
            "        [6559,    6,  257,  ...,    0,    0,    0]])\n",
            "Labels: tensor([1, 3, 2, 3, 1, 2, 3, 3, 1, 1, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 1, 2, 1, 3,\n",
            "        2, 1, 2, 2, 2, 1, 1, 3])\n",
            "Data: tensor([[ 170, 8214,  569,  ...,    0,    0,    0],\n",
            "        [  21,  505,    0,  ...,    0,    0,    0],\n",
            "        [ 606,  361,   33,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [1998,  107,   22,  ...,    0,    0,    0],\n",
            "        [2683,  557, 1288,  ...,    0,    0,    0],\n",
            "        [   4,    4, 1845,  ...,    0,    0,    0]])\n",
            "Labels: tensor([3, 1, 1, 1, 1, 2, 3, 1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 1, 1, 2, 1, 1, 2, 1,\n",
            "        2, 3, 3, 1, 2, 2, 2, 2])\n",
            "Data: tensor([[  62,  101,   74,  ...,    0,    0,    0],\n",
            "        [  25,  253,  239,  ...,    0,    0,    0],\n",
            "        [1218,  130,  324,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [9747, 7929,  694,  ...,    0,    0,    0],\n",
            "        [   7,  592, 4629,  ...,    0,    0,    0],\n",
            "        [  61,   41,  111,  ...,    0,    0,    0]])\n",
            "Labels: tensor([1, 1, 3, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 2, 1, 2, 1, 1, 1, 2,\n",
            "        3, 1, 3, 3, 3, 1, 2, 1])\n",
            "Data: tensor([[  673,    67,  9822,  ...,     0,     0,     0],\n",
            "        [  116,   397,     0,  ...,     0,     0,     0],\n",
            "        [    0,   235, 12652,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [ 2537,     6,    24,  ...,     0,     0,     0],\n",
            "        [ 2538,   252,     8,  ...,     0,     0,     0],\n",
            "        [ 1696,  2189,  3990,  ...,     0,     0,     0]])\n",
            "Labels: tensor([1, 1, 1, 3, 3, 1, 3, 1, 3, 3, 3, 3, 2, 1, 2, 1, 2, 3, 1, 2, 1, 2, 1, 1,\n",
            "        3, 2, 3, 3, 1, 1, 2, 3])\n",
            "Data: tensor([[ 297,   47, 6941,  ...,    0,    0,    0],\n",
            "        [ 442,  199,    0,  ...,    0,    0,    0],\n",
            "        [ 297,  318,  145,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 138,  167, 1163,  ...,    0,    0,    0],\n",
            "        [ 268,   51, 2931,  ...,    0,    0,    0],\n",
            "        [  90,   14,  263,  ...,    0,    0,    0]])\n",
            "Labels: tensor([2, 3, 3, 1, 2, 2, 3, 3, 2, 2, 2, 1, 3, 1, 1, 2, 2, 3, 2, 3, 1, 1, 3, 2,\n",
            "        2, 2, 2, 1, 1, 1, 3, 2])\n",
            "Data: tensor([[   60,   383,   391,  ...,     0,     0,     0],\n",
            "        [   66,  2067,     1,  ...,     0,     0,     0],\n",
            "        [ 3769,     0,     0,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [ 3640,    73,   170,  ...,     0,     0,     0],\n",
            "        [  953,   102, 15359,  ...,     0,     0,     0],\n",
            "        [ 1911,   259,  2192,  ...,     0,     0,     0]])\n",
            "Labels: tensor([1, 3, 1, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 3, 1, 2, 3, 1, 1, 2, 1, 3, 2, 2,\n",
            "        1, 1, 3, 1, 3, 2, 2, 2])\n",
            "Data: tensor([[  288,   213,   122,  ...,     0,     0,     0],\n",
            "        [    3,    37,  3040,  ...,     0,     0,     0],\n",
            "        [  135,   104,  1310,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [11466,    67,   159,  ...,     0,     0,     0],\n",
            "        [ 1328,  7098,  2025,  ...,     0,     0,     0],\n",
            "        [   36,    17,     2,  ...,     0,     0,     0]])\n",
            "Labels: tensor([2, 2, 3, 2, 2, 3, 3, 2, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 3, 2, 2, 1, 3, 3,\n",
            "        1, 2, 3, 3, 3, 2, 2, 2])\n",
            "Data: tensor([[   4,    4,  981,  ...,    0,    0,    0],\n",
            "        [3177, 7111,   44,  ...,    0,    0,    0],\n",
            "        [  44,    7,  468,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 112, 3238, 1291,  ...,    0,    0,    0],\n",
            "        [6703, 9487,  563,  ...,    0,    0,    0],\n",
            "        [  39,    2,  808,  ...,    0,    0,    0]])\n",
            "Labels: tensor([1, 3, 2, 2, 1, 1, 2, 2, 2, 2, 3, 2, 3, 1, 2, 1, 2, 1, 2, 2, 3, 1, 2, 1,\n",
            "        3, 2, 2, 1, 1, 2, 2, 1])\n",
            "Data: tensor([[  135, 12852, 29313,  ...,     0,     0,     0],\n",
            "        [  557,  5657,   222,  ...,     0,     0,     0],\n",
            "        [  628,  2472,   112,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [ 2324,  1082,    55,  ...,     0,     0,     0],\n",
            "        [   28,    66,  6610,  ...,     0,     0,     0],\n",
            "        [  628,  4341,    29,  ...,     0,     0,     0]])\n",
            "Labels: tensor([3, 2, 3, 2, 3, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1, 3, 1, 1, 2, 3, 3, 3,\n",
            "        1, 2, 2, 2, 3, 3, 2, 3])\n",
            "Data: tensor([[ 168,  289,  299,  ...,    0,    0,    0],\n",
            "        [  57, 1041,  233,  ...,    0,    0,    0],\n",
            "        [ 116,  204,   84,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [  88,   83,  403,  ...,    0,    0,    0],\n",
            "        [1552,  687,    0,  ...,    0,    0,    0],\n",
            "        [  95,   15, 1158,  ...,    0,    0,    0]])\n",
            "Labels: tensor([1, 2, 1, 2, 1, 2, 2, 1, 1, 3, 2, 1, 3, 2, 1, 3, 3, 2, 2, 1, 3, 2, 1, 3,\n",
            "        2, 3, 2, 3, 1, 1, 3, 1])\n",
            "Data: tensor([[1022, 6893, 9803,  ...,    0,    0,    0],\n",
            "        [ 164, 1183,  240,  ...,    0,    0,    0],\n",
            "        [8675,    4,    4,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [6713, 5184, 1618,  ...,    0,    0,    0],\n",
            "        [  27,  186,   27,  ...,    0,    0,    0],\n",
            "        [ 136,    1, 3341,  ...,    0,    0,    0]])\n",
            "Labels: tensor([1, 2, 2, 1, 1, 2, 2, 3, 3, 3, 1, 2, 3, 3, 1, 3, 2, 2, 3, 1, 1, 2, 3, 2,\n",
            "        3, 2, 2, 2, 2, 2, 1, 2])\n",
            "Data: tensor([[  39,  175,   39,  ...,    0,    0,    0],\n",
            "        [  36,   26,   33,  ...,    0,    0,    0],\n",
            "        [ 178, 9655,  319,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 724,  904,  305,  ...,    0,    0,    0],\n",
            "        [  48,   42, 1717,  ...,    0,    0,    0],\n",
            "        [ 848, 2380, 3198,  ...,    0,    0,    0]])\n",
            "Labels: tensor([1, 1, 2, 3, 2, 2, 3, 2, 2, 2, 2, 1, 1, 2, 1, 3, 2, 2, 1, 1, 1, 3, 1, 2,\n",
            "        1, 2, 3, 3, 3, 2, 1, 1])\n",
            "Data: tensor([[ 140,  362,  247,  ...,    0,    0,    0],\n",
            "        [1739,    5,   50,  ...,    0,    0,    0],\n",
            "        [2807, 1326,  416,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 225,  431, 2605,  ...,    0,    0,    0],\n",
            "        [  79,   16,  307,  ...,    0,    0,    0],\n",
            "        [7886,  418,  463,  ...,    0,    0,    0]])\n",
            "Labels: tensor([3, 3, 2, 2, 1, 2, 1, 2, 1, 1, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 3, 2, 2,\n",
            "        2, 1, 1, 1, 3, 1, 2, 2])\n",
            "Data: tensor([[ 352, 2202, 3591,  ...,    0,    0,    0],\n",
            "        [4309, 3433, 1463,  ...,    0,    0,    0],\n",
            "        [4345,   20, 3001,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [1190,  324,  238,  ...,    0,    0,    0],\n",
            "        [1974,   32,  123,  ...,    0,    0,    0],\n",
            "        [ 270,   16,  400,  ...,    0,    0,    0]])\n",
            "Labels: tensor([2, 2, 3, 1, 1, 2, 2, 3, 3, 1, 2, 3, 2, 3, 3, 3, 2, 1, 2, 1, 2, 1, 2, 1,\n",
            "        2, 3, 2, 3, 1, 2, 2, 2])\n",
            "Data: tensor([[   47,  1443,   816,  ...,     0,     0,     0],\n",
            "        [  293,   836,   118,  ...,     0,     0,     0],\n",
            "        [  661, 10295,   661,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [ 1158,   893,  7121,  ...,     0,     0,     0],\n",
            "        [  290,  3592,  3820,  ...,     0,     0,     0],\n",
            "        [  298,    46,    14,  ...,     0,     0,     0]])\n",
            "Labels: tensor([1, 1, 2, 3, 2, 3, 1, 1, 2, 2, 3, 3, 1, 1, 1, 1, 2, 3, 1, 1, 3, 2, 2, 1,\n",
            "        1, 3, 2, 1, 2, 1, 2, 2])\n",
            "Data: tensor([[  94,  823,  306,  ...,    0,    0,    0],\n",
            "        [1065, 4623, 5211,  ...,    0,    0,    0],\n",
            "        [  14,   45,   24,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [1920,  247, 2275,  ...,    0,    0,    0],\n",
            "        [ 678, 2623,  436,  ...,    0,    0,    0],\n",
            "        [ 420, 6903, 3039,  ...,    0,    0,    0]])\n",
            "Labels: tensor([2, 2, 2, 3, 1, 2, 2, 2, 1, 3, 2, 3, 1, 1, 2, 2, 3, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
        "        super(SentimentRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.to(torch.float32)\n",
        "\n",
        "    def forward(self, x, h0=None, c0=None):\n",
        "        # If the initial hidden and cell states are not provided, initialize them with zeros\n",
        "        if h0 is None:\n",
        "            h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device).to(x.dtype)\n",
        "        if c0 is None:\n",
        "            c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device).to(x.dtype)\n",
        "\n",
        "        # Add an additional dimension for sequence_length\n",
        "        x = x.unsqueeze(1)\n",
        "\n",
        "        # Pass the input through the LSTM layer\n",
        "        out, _ = self.rnn(x, (h0, c0))\n",
        "\n",
        "        # Extract the output of the last time step\n",
        "        out = out[:, -1, :]\n",
        "\n",
        "        # Pass the output through the fully connected layer for sentiment prediction\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "def train(model, data_loader, learning_rate):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Check if GPU is available\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    num_epochs = 10\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "\n",
        "        total_loss = 0.0\n",
        "        # for data, labels in data_loader:\n",
        "        #     print(data)\n",
        "        #     print(labels)\n",
        "        #     # data, labels = data.to(device), labels.to(device)\n",
        "\n",
        "        #     # # Forward pass\n",
        "        #     predictions = model(data)\n",
        "        #     print(predictions)\n",
        "            # # Compute the loss\n",
        "            # loss = criterion(predictions, labels)\n",
        "\n",
        "            # # Backpropagation and optimization\n",
        "            # optimizer.zero_grad()\n",
        "            # loss.backward()\n",
        "            # optimizer.step()\n",
        "\n",
        "            # total_loss += loss.item()\n",
        "\n",
        "        # Calculate average loss for the epoch\n",
        "        # average_loss = total_loss / len(data_loader)\n",
        "\n",
        "        # print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {average_loss:.4f}\")\n",
        "        for data, labels in data_loader:\n",
        "            data, labels = data.to(device), labels.float().to(device)\n",
        "\n",
        "            # Initialize the hidden and cell states for each batch\n",
        "            h0 = torch.zeros(model.num_layers, data.size(0), model.hidden_size).to(device).to(data.dtype)\n",
        "            c0 = torch.zeros(model.num_layers, data.size(0), model.hidden_size).to(device).to(data.dtype)\n",
        "\n",
        "            # Forward pass\n",
        "            predictions = model(data, h0, c0)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(predictions, labels)\n",
        "\n",
        "            # Backpropagation and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n"
      ],
      "metadata": {
        "id": "gJyUUxYntFEk"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(SentimentRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Define the LSTM layer\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        # Define the fully connected layer for sentiment prediction\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "      # Initialize hidden and cell states\n",
        "      h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device).to(self.lstm.weight_ih_l0.dtype)\n",
        "      c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device).to(self.lstm.weight_ih_l0.dtype)\n",
        "\n",
        "      # Convert input tensor to the same data type as the LSTM weights\n",
        "      x = x.to(self.lstm.weight_ih_l0.dtype)\n",
        "      out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "      # Extract the output of the last time step\n",
        "      out = out[:, -1, :]\n",
        "\n",
        "      # Pass the output through the fully connected layer for sentiment prediction\n",
        "      out = self.fc(out)\n",
        "\n",
        "      return out\n",
        "\n",
        "def train(model, train_dataloader, learning_rate):\n",
        "    device = torch.device(\"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    epochs = 10\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "\n",
        "        for batch in train_dataloader:\n",
        "            # Move the batch to the device\n",
        "            inputs = batch[0].to(device)\n",
        "            labels = batch[1].to(device)\n",
        "            # print(inputs)\n",
        "            # print(labels)\n",
        "        #     # Reshape the inputs to match the expected shape\n",
        "            inputs = inputs.unsqueeze(1)  # Add an extra dimension for sequence length\n",
        "            # print(inputs)\n",
        "            # Clear the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        #     # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "        #     # Reshape the outputs and labels\n",
        "            outputs = outputs.view(-1, outputs.size(-1))\n",
        "            labels = labels.view(-1)\n",
        "\n",
        "        #     # Convert labels to torch.long\n",
        "            labels = labels.to(torch.long)\n",
        "        #     # Calculate the loss\n",
        "            loss = loss_fn(outputs, labels)\n",
        "\n",
        "        #     # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        #     # Update the running loss\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        #     # Calculate the number of correct predictions\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "            total_predictions += labels.size(0)\n",
        "\n",
        "        # # Calculate the training accuracy\n",
        "        training_accuracy = correct_predictions / total_predictions\n",
        "\n",
        "        # # Print the average loss and accuracy for the epoch\n",
        "        epoch_loss = running_loss / len(train_dataloader)\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss}, Accuracy: {training_accuracy * 100}%\")"
      ],
      "metadata": {
        "id": "NgIa857Y_J60"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_model = SentimentRNN(198,64, 1, 4)\n",
        "train(first_model, train_data_loader, 0.0001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUdAqGYPtlNb",
        "outputId": "0ef962ad-b059-46a4-9b3c-b30f03f551a9"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.3892389701187977, Accuracy: 28.599374772252162%\n",
            "Epoch [2/10], Loss: 1.3511371712011793, Accuracy: 33.401737596133565%\n",
            "Epoch [3/10], Loss: 1.3239180294282598, Accuracy: 36.10210774630329%\n",
            "Epoch [4/10], Loss: 1.3009230779723886, Accuracy: 37.507911240674325%\n",
            "Epoch [5/10], Loss: 1.2829865894434642, Accuracy: 38.731516464969985%\n",
            "Epoch [6/10], Loss: 1.2660110488991065, Accuracy: 39.44880228610882%\n",
            "Epoch [7/10], Loss: 1.2506975177606923, Accuracy: 40.04909763909399%\n",
            "Epoch [8/10], Loss: 1.2372837141247615, Accuracy: 40.78364434897681%\n",
            "Epoch [9/10], Loss: 1.225201642220737, Accuracy: 41.360925183636674%\n",
            "Epoch [10/10], Loss: 1.215578316837732, Accuracy: 41.665867551447036%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgV5lWfAiJxk",
        "outputId": "1014aa9a-cf6f-4def-ff1c-990fd8d54b62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25457"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    }
  ]
}