{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"3n4_p_iHY2_s","executionInfo":{"status":"ok","timestamp":1691116391124,"user_tz":240,"elapsed":6715,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"}},"outputId":"29cfbef7-a5d8-4176-f967-f834e00bc664"},"outputs":[{"output_type":"stream","name":"stdout","text":["CUDA is not available.\n"]}],"source":["import torch\n","\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print(f\"CUDA device name: {torch.cuda.get_device_name()}\")\n","else:\n","    print(\"CUDA is not available.\")"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iYJzPFoQGQoj","outputId":"7a21756f-1be0-4bed-d98b-204b6154b242","tags":[],"executionInfo":{"status":"ok","timestamp":1691116410750,"user_tz":240,"elapsed":19658,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting emoji\n","  Downloading emoji-2.7.0.tar.gz (361 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m361.8/361.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: emoji\n","  Building wheel for emoji (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-2.7.0-py2.py3-none-any.whl size=356563 sha256=77c9c6e1908574881e373dd7a504ebcbaebc61cfe0ae7a6502ffdabc3403c06f\n","  Stored in directory: /root/.cache/pip/wheels/41/11/48/5df0b9727d5669c9174a141134f10304d1d78a3b89a4676f3d\n","Successfully built emoji\n","Installing collected packages: emoji\n","Successfully installed emoji-2.7.0\n"]}],"source":["pip install emoji --upgrade"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BPtrjX9yGW50","outputId":"ac5b87cb-0431-4111-c866-612222ee8766","tags":[],"executionInfo":{"status":"ok","timestamp":1691116415335,"user_tz":240,"elapsed":4593,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.6)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.1)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"]}],"source":["!pip install nltk"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qbZESrVGGYg7","outputId":"f06b6409-1dbf-4128-c229-61fd0ed94757","tags":[],"executionInfo":{"status":"ok","timestamp":1691116422507,"user_tz":240,"elapsed":7176,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting contractions\n","  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n","Collecting textsearch>=0.0.21 (from contractions)\n","  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n","Collecting anyascii (from textsearch>=0.0.21->contractions)\n","  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n","  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n","Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n"]}],"source":["!pip install contractions"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IXuI5rbGGZor","outputId":"b4743b69-bdb7-438d-d0b0-cebe916ff84c","tags":[],"executionInfo":{"status":"ok","timestamp":1691116427068,"user_tz":240,"elapsed":4566,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}],"source":["import pandas as pd\n","import re\n","import emoji\n","\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk.sentiment.util import mark_negation\n","import nltk\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","import contractions"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nZiKxYPmGa-j","outputId":"59483470-294f-43ef-83c5-a807a91dc883","tags":[],"executionInfo":{"status":"ok","timestamp":1691116427282,"user_tz":240,"elapsed":219,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":6}],"source":["nltk.download('wordnet')"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"id":"2jBK6ciHGm1e","outputId":"41f5af87-a400-45ba-eb36-ca42fbf92bb6","tags":[],"executionInfo":{"status":"error","timestamp":1691116427742,"user_tz":240,"elapsed":466,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"}}},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-9b1bd36a354b>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Note: ideally data doesnt have column headers but we add them to easy manipulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"company\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sentiment\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tweet\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdf_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'twitter_training.csv'"]}],"source":["# Loading the data\n","train_path = \"twitter_training.csv\"\n","val_path = \"twitter_validation.csv\"\n","# Note: ideally data doesnt have column headers but we add them to easy manipulation\n","headers = [\"id\", \"company\", \"sentiment\", \"tweet\"]\n","df_train = pd.read_csv(train_path, names=headers)\n","df_val = pd.read_csv(val_path, names=headers)\n","\n","print(df_train.head())\n","print(df_val.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HX2bpl4AGzhP","tags":[],"executionInfo":{"status":"aborted","timestamp":1691116427763,"user_tz":240,"elapsed":59,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"}}},"outputs":[],"source":["abbreviations = {\n","    \"$\" : \" dollar \",\n","    \"€\" : \" euro \",\n","    \"4ao\" : \"for adults only\",\n","    \"a.m\" : \"before midday\",\n","    \"a3\" : \"anytime anywhere anyplace\",\n","    \"aamof\" : \"as a matter of fact\",\n","    \"acct\" : \"account\",\n","    \"adih\" : \"another day in hell\",\n","    \"afaic\" : \"as far as i am concerned\",\n","    \"afaict\" : \"as far as i can tell\",\n","    \"afaik\" : \"as far as i know\",\n","    \"afair\" : \"as far as i remember\",\n","    \"afk\" : \"away from keyboard\",\n","    \"app\" : \"application\",\n","    \"approx\" : \"approximately\",\n","    \"apps\" : \"applications\",\n","    \"asap\" : \"as soon as possible\",\n","    \"asl\" : \"age, sex, location\",\n","    \"atk\" : \"at the keyboard\",\n","    \"ave.\" : \"avenue\",\n","    \"aymm\" : \"are you my mother\",\n","    \"ayor\" : \"at your own risk\",\n","    \"b&b\" : \"bed and breakfast\",\n","    \"b+b\" : \"bed and breakfast\",\n","    \"b.c\" : \"before christ\",\n","    \"b2b\" : \"business to business\",\n","    \"b2c\" : \"business to customer\",\n","    \"b4\" : \"before\",\n","    \"b4n\" : \"bye for now\",\n","    \"b@u\" : \"back at you\",\n","    \"bae\" : \"before anyone else\",\n","    \"bak\" : \"back at keyboard\",\n","    \"bbbg\" : \"bye bye be good\",\n","    \"bbc\" : \"british broadcasting corporation\",\n","    \"bbias\" : \"be back in a second\",\n","    \"bbl\" : \"be back later\",\n","    \"bbs\" : \"be back soon\",\n","    \"be4\" : \"before\",\n","    \"bfn\" : \"bye for now\",\n","    \"blvd\" : \"boulevard\",\n","    \"bout\" : \"about\",\n","    \"brb\" : \"be right back\",\n","    \"bros\" : \"brothers\",\n","    \"brt\" : \"be right there\",\n","    \"bsaaw\" : \"big smile and a wink\",\n","    \"btw\" : \"by the way\",\n","    \"bwl\" : \"bursting with laughter\",\n","    \"c/o\" : \"care of\",\n","    \"cet\" : \"central european time\",\n","    \"cf\" : \"compare\",\n","    \"cia\" : \"central intelligence agency\",\n","    \"csl\" : \"can not stop laughing\",\n","    \"cu\" : \"see you\",\n","    \"cul8r\" : \"see you later\",\n","    \"cv\" : \"curriculum vitae\",\n","    \"cwot\" : \"complete waste of time\",\n","    \"cya\" : \"see you\",\n","    \"cyt\" : \"see you tomorrow\",\n","    \"dae\" : \"does anyone else\",\n","    \"dbmib\" : \"do not bother me i am busy\",\n","    \"diy\" : \"do it yourself\",\n","    \"dm\" : \"direct message\",\n","    \"dwh\" : \"during work hours\",\n","    \"e123\" : \"easy as one two three\",\n","    \"eet\" : \"eastern european time\",\n","    \"eg\" : \"example\",\n","    \"embm\" : \"early morning business meeting\",\n","    \"encl\" : \"enclosed\",\n","    \"encl.\" : \"enclosed\",\n","    \"etc\" : \"and so on\",\n","    \"faq\" : \"frequently asked questions\",\n","    \"fawc\" : \"for anyone who cares\",\n","    \"fb\" : \"facebook\",\n","    \"fc\" : \"fingers crossed\",\n","    \"fig\" : \"figure\",\n","    \"fimh\" : \"forever in my heart\",\n","    \"ft.\" : \"feet\",\n","    \"ft\" : \"featuring\",\n","    \"ftl\" : \"for the loss\",\n","    \"ftw\" : \"for the win\",\n","    \"fwiw\" : \"for what it is worth\",\n","    \"fyi\" : \"for your information\",\n","    \"g9\" : \"genius\",\n","    \"gahoy\" : \"get a hold of yourself\",\n","    \"gal\" : \"get a life\",\n","    \"gcse\" : \"general certificate of secondary education\",\n","    \"gfn\" : \"gone for now\",\n","    \"gg\" : \"good game\",\n","    \"gl\" : \"good luck\",\n","    \"glhf\" : \"good luck have fun\",\n","    \"gmt\" : \"greenwich mean time\",\n","    \"gmta\" : \"great minds think alike\",\n","    \"gn\" : \"good night\",\n","    \"g.o.a.t\" : \"greatest of all time\",\n","    \"goat\" : \"greatest of all time\",\n","    \"goi\" : \"get over it\",\n","    \"gps\" : \"global positioning system\",\n","    \"gr8\" : \"great\",\n","    \"gratz\" : \"congratulations\",\n","    \"gyal\" : \"girl\",\n","    \"h&c\" : \"hot and cold\",\n","    \"hp\" : \"horsepower\",\n","    \"hr\" : \"hour\",\n","    \"hrh\" : \"his royal highness\",\n","    \"ht\" : \"height\",\n","    \"ibrb\" : \"i will be right back\",\n","    \"ic\" : \"i see\",\n","    \"icq\" : \"i seek you\",\n","    \"icymi\" : \"in case you missed it\",\n","    \"idc\" : \"i do not care\",\n","    \"idgadf\" : \"i do not give a damn fuck\",\n","    \"idgaf\" : \"i do not give a fuck\",\n","    \"idk\" : \"i do not know\",\n","    \"ie\" : \"that is\",\n","    \"i.e\" : \"that is\",\n","    \"ifyp\" : \"i feel your pain\",\n","    \"IG\" : \"instagram\",\n","    \"iirc\" : \"if i remember correctly\",\n","    \"ilu\" : \"i love you\",\n","    \"ily\" : \"i love you\",\n","    \"imho\" : \"in my humble opinion\",\n","    \"imo\" : \"in my opinion\",\n","    \"imu\" : \"i miss you\",\n","    \"iow\" : \"in other words\",\n","    \"irl\" : \"in real life\",\n","    \"j4f\" : \"just for fun\",\n","    \"jic\" : \"just in case\",\n","    \"jk\" : \"just kidding\",\n","    \"jsyk\" : \"just so you know\",\n","    \"l8r\" : \"later\",\n","    \"lb\" : \"pound\",\n","    \"lbs\" : \"pounds\",\n","    \"ldr\" : \"long distance relationship\",\n","    \"lmao\" : \"laugh my ass off\",\n","    \"lmfao\" : \"laugh my fucking ass off\",\n","    \"lol\" : \"laughing out loud\",\n","    \"ltd\" : \"limited\",\n","    \"ltns\" : \"long time no see\",\n","    \"m8\" : \"mate\",\n","    \"mf\" : \"motherfucker\",\n","    \"mfs\" : \"motherfuckers\",\n","    \"mfw\" : \"my face when\",\n","    \"mofo\" : \"motherfucker\",\n","    \"mph\" : \"miles per hour\",\n","    \"mr\" : \"mister\",\n","    \"mrw\" : \"my reaction when\",\n","    \"ms\" : \"miss\",\n","    \"mte\" : \"my thoughts exactly\",\n","    \"nagi\" : \"not a good idea\",\n","    \"nbc\" : \"national broadcasting company\",\n","    \"nbd\" : \"not big deal\",\n","    \"nfs\" : \"not for sale\",\n","    \"ngl\" : \"not going to lie\",\n","    \"nhs\" : \"national health service\",\n","    \"nrn\" : \"no reply necessary\",\n","    \"nsfl\" : \"not safe for life\",\n","    \"nsfw\" : \"not safe for work\",\n","    \"nth\" : \"nice to have\",\n","    \"nvr\" : \"never\",\n","    \"nyc\" : \"new york city\",\n","    \"oc\" : \"original content\",\n","    \"og\" : \"original\",\n","    \"ohp\" : \"overhead projector\",\n","    \"oic\" : \"oh i see\",\n","    \"omdb\" : \"over my dead body\",\n","    \"omg\" : \"oh my god\",\n","    \"omw\" : \"on my way\",\n","    \"p.a\" : \"per annum\",\n","    \"p.m\" : \"after midday\",\n","    \"pm\" : \"prime minister\",\n","    \"poc\" : \"people of color\",\n","    \"pov\" : \"point of view\",\n","    \"pp\" : \"pages\",\n","    \"ppl\" : \"people\",\n","    \"prw\" : \"parents are watching\",\n","    \"ps\" : \"postscript\",\n","    \"pt\" : \"point\",\n","    \"ptb\" : \"please text back\",\n","    \"pto\" : \"please turn over\",\n","    \"qpsa\" : \"what happens\", #\"que pasa\",\n","    \"ratchet\" : \"rude\",\n","    \"rbtl\" : \"read between the lines\",\n","    \"rlrt\" : \"real life retweet\",\n","    \"rofl\" : \"rolling on the floor laughing\",\n","    \"roflol\" : \"rolling on the floor laughing out loud\",\n","    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n","    \"rt\" : \"retweet\",\n","    \"ruok\" : \"are you ok\",\n","    \"sfw\" : \"safe for work\",\n","    \"sk8\" : \"skate\",\n","    \"smh\" : \"shake my head\",\n","    \"sq\" : \"square\",\n","    \"srsly\" : \"seriously\",\n","    \"ssdd\" : \"same stuff different day\",\n","    \"tbh\" : \"to be honest\",\n","    \"tbs\" : \"tablespooful\",\n","    \"tbsp\" : \"tablespooful\",\n","    \"tfw\" : \"that feeling when\",\n","    \"thks\" : \"thank you\",\n","    \"tho\" : \"though\",\n","    \"thx\" : \"thank you\",\n","    \"tia\" : \"thanks in advance\",\n","    \"til\" : \"today i learned\",\n","    \"tl;dr\" : \"too long i did not read\",\n","    \"tldr\" : \"too long i did not read\",\n","    \"tmb\" : \"tweet me back\",\n","    \"tntl\" : \"trying not to laugh\",\n","    \"ttyl\" : \"talk to you later\",\n","    \"u\" : \"you\",\n","    \"u2\" : \"you too\",\n","    \"u4e\" : \"yours for ever\",\n","    \"utc\" : \"coordinated universal time\",\n","    \"w/\" : \"with\",\n","    \"w/o\" : \"without\",\n","    \"w8\" : \"wait\",\n","    \"wassup\" : \"what is up\",\n","    \"wb\" : \"welcome back\",\n","    \"wtf\" : \"what the fuck\",\n","    \"wtg\" : \"way to go\",\n","    \"wtpa\" : \"where the party at\",\n","    \"wuf\" : \"where are you from\",\n","    \"wuzup\" : \"what is up\",\n","    \"wywh\" : \"wish you were here\",\n","    \"yd\" : \"yard\",\n","    \"ygtr\" : \"you got that right\",\n","    \"ynk\" : \"you never know\",\n","    \"zzz\" : \"sleeping bored and tired\"\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H9qMwKl5G1Is","tags":[],"executionInfo":{"status":"aborted","timestamp":1691116427767,"user_tz":240,"elapsed":61,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"}}},"outputs":[],"source":["# Step 1: Remove NaN values\n","def remove_nan_values(df, columns):\n","    for column in columns:\n","        df = df[df[column].notna()]  # Remove rows with NaN values\n","        df = df[df[column].astype(bool)]  # Remove rows with empty lists or lists with only white spaces\n","    return df\n","\n","# Step 2: Assign tweet labels\n","def one_hot_encode_sentiments(sentiment): #make neutral and irrelavent same number, and look over\n","    if sentiment == 'Positive':\n","        return 2\n","    elif sentiment == 'Negative':\n","        return 0\n","    elif sentiment == 'Neutral':\n","        return 1\n","    elif sentiment == 'Irrelevant':\n","        return 1\n","\n","def expand_slang(text):\n","    tokens = word_tokenize(text)\n","    tokens = [abbreviations.get(word.lower(), word) for word in tokens]\n","    text = ' '.join(tokens)\n","    return text\n","\n","\n","# Step 3: Clean tweets\n","def clean_tweets(tweet):\n","    # Remove @mentions\n","    tweet = re.sub(r'@\\w+', '', tweet)\n","\n","    # Remove URLs\n","    tweet = re.sub(r'http\\S+|www\\S+', '', tweet)\n","\n","    # Remove hashtags\n","    tweet = re.sub(r'#\\w+', '', tweet)\n","\n","    # Convert emojis to text\n","    tweet = emoji.demojize(tweet)\n","\n","    # Expand contractions\n","    tweet = contractions.fix(tweet)\n","    ##############################3\n","\n","    # Remove numbers\n","    tweet = re.sub(r'\\d+', '', tweet)\n","\n","    # Remove punctuation\n","    tweet = re.sub(r'[^\\w\\s]', '', tweet)\n","\n","    # Remove excessive whitespace\n","    tweet = re.sub(r'\\s+', ' ', tweet)\n","\n","    # Standardize repeated characters\n","    tweet = re.sub(r'(.)\\1+', r'\\1\\1', tweet)\n","\n","    #Expand slang terms\n","    tweet = expand_slang(tweet)\n","\n","    # Tokenize the tweet\n","    tokens = word_tokenize(tweet)\n","\n","    # Lowercase and normalize\n","    tokens = [token.lower() for token in tokens]\n","\n","    # Remove stop words\n","    stop_words = set(stopwords.words('english'))\n","    tokens = [token for token in tokens if token.lower() not in stop_words]\n","\n","    # Handle abbreviations and acronyms\n","    abbreviation_words = {\"lol\": \"laughing out loud\", \"btw\": \"by the way\"}  # Add more as needed\n","    tokens = [abbreviation_words.get(token.lower(), token) for token in tokens]\n","\n","    # Lemmatization\n","    lemmatizer = WordNetLemmatizer()\n","    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n","\n","    # Handle negation using mark_negation\n","    #tokens = mark_negation(tokens)\n","    #tweet = \"I don't like this product. It is not good.\"\n","    #['I', \"don't\", 'like_NEG', 'this_NEG', 'product_NEG', '.', 'It_NEG', 'is_NEG', 'not_NEG', 'good_NEG', '.']\n","\n","    # Uncomment the following lines to include POS tags as comments\n","    #pos_tags = nltk.pos_tag(tokens)\n","    #tokens_with_pos = [f\"{token}/{pos}\" for token, pos in pos_tags]\n","\n","    return tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o4XLR8RRG7PD","tags":[],"executionInfo":{"status":"aborted","timestamp":1691116427770,"user_tz":240,"elapsed":63,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"}}},"outputs":[],"source":["# Step 1: Remove NaN values\n","columns_to_check = [\"id\", \"company\", \"sentiment\", \"tweet\"]\n","df_train = remove_nan_values(df_train, columns_to_check)\n","df_val = remove_nan_values(df_val, columns_to_check)\n","\n","print(df_train.head())\n","print(df_val.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"obq9v6IMG8i9","tags":[],"executionInfo":{"status":"aborted","timestamp":1691116427772,"user_tz":240,"elapsed":64,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"}}},"outputs":[],"source":["# Step 2: Assign tweet labels and remove rows with 'Irrelevant' sentiment\n","df_train['sentiment'] = df_train['sentiment'].apply(one_hot_encode_sentiments)\n","df_val['sentiment'] = df_val['sentiment'].apply(one_hot_encode_sentiments)\n","\n","# Remove rows with 'Irrelevant' sentiment from the DataFrame\n","df_train = df_train.dropna(subset=['sentiment'])\n","df_val = df_val.dropna(subset=['sentiment'])\n","\n","print(df_train.head())\n","print(df_val.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W_5aw4nVG91C","tags":[],"executionInfo":{"status":"aborted","timestamp":1691116427781,"user_tz":240,"elapsed":73,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"}}},"outputs":[],"source":["# Step 3: Clean tweets\n","df_train['tweet'] = df_train['tweet'].apply(clean_tweets)\n","df_val['tweet'] = df_val['tweet'].apply(clean_tweets)\n","\n","print(df_train.head())\n","print(df_val.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bv96Y-HCG_XA","tags":[],"executionInfo":{"status":"aborted","timestamp":1691116427783,"user_tz":240,"elapsed":74,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"}}},"outputs":[],"source":["# Step 1: Remove NaN values\n","columns_to_check = [\"id\", \"company\", \"sentiment\", \"tweet\"]\n","df_train = remove_nan_values(df_train, columns_to_check)\n","df_val = remove_nan_values(df_val, columns_to_check)\n","\n","print(df_train.head())\n","print(df_val.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zD3zM14xHAlZ","tags":[],"executionInfo":{"status":"aborted","timestamp":1691116427785,"user_tz":240,"elapsed":75,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"}}},"outputs":[],"source":["# Print the resulting DataFrame\n","print(df_train.head())\n","print(df_val.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OqMQesJiHCzv","tags":[],"executionInfo":{"status":"aborted","timestamp":1691116427786,"user_tz":240,"elapsed":75,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"}}},"outputs":[],"source":["# Assuming your DataFrame is named df\n","import torch\n","# Drop the 'id' column\n","df_train_new = df_train.drop('id', axis=1)\n","df_val_new = df_val.drop('id', axis=1)\n","\n","# Export the DataFrame to a CSV file\n","df_train_new.to_csv('df_train_new_int.csv', index=False)\n","df_val_new.to_csv('df_val_new_int.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NojZ5_e5HEhs","tags":[],"executionInfo":{"status":"aborted","timestamp":1691116427788,"user_tz":240,"elapsed":75,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"}}},"outputs":[],"source":["print(df_train_new[:5])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h5Gww7ThyEd8","tags":[],"executionInfo":{"status":"aborted","timestamp":1691116427791,"user_tz":240,"elapsed":74,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"}}},"outputs":[],"source":["import torch\n","import pandas as pd\n","from collections import Counter\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","# Load GloVe embeddings\n","def load_glove_embeddings(embeddings_path):\n","    embeddings_index = {}\n","    with open(embeddings_path, \"r\", encoding=\"utf-8\") as f:\n","        for line in f:\n","            values = line.split()\n","            word = values[0]\n","            coefs = torch.tensor([float(val) for val in values[1:]])\n","            embeddings_index[word] = coefs\n","    return embeddings_index\n","\n","def get_vocab_dict(word_counts):\n","    sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n","    vocab_dict = {word: idx for idx, (word, _) in enumerate(sorted_words)}\n","    return vocab_dict\n","\n","def indices_to_embeddings(indices, embeddings_index, embedding_dim, vocab_dict):\n","    embeddings = []\n","    for idx in indices:\n","        word = list(vocab_dict.keys())[list(vocab_dict.values()).index(idx)]\n","        if word in embeddings_index:\n","            embeddings.append(embeddings_index[word])\n","        else:\n","            embeddings.append(torch.zeros(embedding_dim))\n","    return embeddings\n","\n","# Paths\n","glove_path = \"glove.twitter.27B.100d.txt\"  # Replace with your actual path\n","data_path_train = \"df_train_new_int.csv\"  # Replace with your actual path\n","data_path_valid = \"df_val_new_int.csv\"  # Replace with your actual path\n","\n","# Load data\n","df_train_new = pd.read_csv(data_path_train)\n","df_val_new = pd.read_csv(data_path_valid)\n","\n","# Flatten the list of lists into a single list\n","all_words = [word for sublist in df_train_new['tweet'] for word in sublist]\n","all_words_valid = [word for sublist in df_val_new['tweet'] for word in sublist]\n","\n","# Create a vocabulary dictionary with words ordered by frequency\n","word_counts = Counter(all_words + all_words_valid)\n","vocab_dict = get_vocab_dict(word_counts)\n","\n","# Convert words to indices for each tweet\n","sequences_of_indices_train = [[vocab_dict[word] for word in sublist] for sublist in df_train_new['tweet']]\n","sequences_of_indices_valid = [[vocab_dict[word] for word in sublist] for sublist in df_val_new['tweet']]\n","\n","# Load GloVe embeddings\n","glove_embeddings = load_glove_embeddings(glove_path)\n","\n","# Convert indices to GloVe embeddings\n","embedding_dim = 100  # Assuming you're using 100-dimensional GloVe embeddings\n","\n","train_embeddings = [indices_to_embeddings(seq, glove_embeddings, embedding_dim, vocab_dict) for seq in sequences_of_indices_train]\n","valid_embeddings = [indices_to_embeddings(seq, glove_embeddings, embedding_dim, vocab_dict) for seq in sequences_of_indices_valid]\n","\n","# Ensure train_embeddings and valid_embeddings are lists of lists of tensors\n","train_embeddings = [torch.stack(embedding_list) for embedding_list in train_embeddings]\n","valid_embeddings = [torch.stack(embedding_list) for embedding_list in valid_embeddings]\n","\n","max_train_seq_length = 100\n","\n","# Pad sequences and create tensors\n","padded_train_embeddings = pad_sequence(\n","    [torch.cat((seq, torch.zeros(max_train_seq_length - len(seq), embedding_dim))) if len(seq) < max_train_seq_length else seq[:max_train_seq_length] for seq in train_embeddings],\n","    batch_first=True, padding_value=0.0\n",")\n","\n","padded_valid_embeddings = pad_sequence(\n","    [torch.cat((seq, torch.zeros(max_train_seq_length - len(seq), embedding_dim))) if len(seq) < max_train_seq_length else seq[:max_train_seq_length] for seq in valid_embeddings],\n","    batch_first=True, padding_value=0.0\n",")\n","\n","# Convert labels to tensors\n","train_labels = torch.tensor(df_train_new['sentiment'].values, dtype=torch.long)\n","valid_labels = torch.tensor(df_val_new['sentiment'].values, dtype=torch.long)\n","\n","# Create datasets and data loaders\n","batch_size = 32\n","train_dataset = TensorDataset(padded_train_embeddings, train_labels)\n","valid_dataset = TensorDataset(padded_valid_embeddings, valid_labels)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n","\n","# Print shapes for verification\n","print(\"Train dataset shape:\", padded_train_embeddings.shape)\n","print(\"Valid dataset shape:\", padded_valid_embeddings.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8TYhhS-oJC_4","tags":[],"executionInfo":{"status":"aborted","timestamp":1691116427797,"user_tz":240,"elapsed":43726,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.metrics import confusion_matrix\n","import numpy\n","\n","class SentimentGRU(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, dropout_prob=0.0):\n","        super(SentimentGRU, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","\n","        # Define the 4-layer bidirectional GRU with batch normalization\n","        self.gru = nn.GRU(input_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout_prob, bidirectional=True)\n","        self.batch_norm = nn.BatchNorm1d(hidden_size * 2)  # Batch normalization layer\n","\n","        # Define the fully connected layer for sentiment prediction\n","        self.fc = nn.Linear(hidden_size * 2, 1)\n","\n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n","\n","        x = x.to(self.gru.weight_ih_l0.dtype)\n","\n","        # Permute dimensions to (batch_size, input_size, sequence_length)\n","        x = x.permute(0, 2, 1)\n","\n","        out, _ = self.gru(x, h0)\n","        out = self.batch_norm(out.permute(0, 2, 1)).permute(0, 2, 1)  # Apply batch normalization\n","\n","        out = out[:, -1, :]\n","\n","        out = self.fc(out)\n","\n","        return out\n","\n","def train(model, train_dataloader, learning_rate, epochs, optimizer):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","\n","    optimizer = optimizer\n","    loss_fn = nn.BCEWithLogitsLoss()\n","\n","    if torch.cuda.is_available():\n","        device = torch.device(\"cuda\")\n","        print(f\"CUDA device name: {torch.cuda.get_device_name()}\")\n","    else:\n","        print(\"CUDA is not available.\")\n","\n","    for epoch in range(epochs):\n","        running_loss = 0.0\n","        correct_predictions = 0\n","        total_predictions = 0\n","\n","        for batch in train_dataloader:\n","            inputs = batch[0].to(device)\n","            labels = batch[1].to(device).float()\n","\n","            # Permute dimensions to (batch_size, input_size, sequence_length)\n","            inputs = inputs.permute(0, 2, 1)\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            outputs = outputs.view(-1, 1)\n","\n","            labels = labels.view(-1, 1)\n","            loss = loss_fn(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","\n","            predicted_probs = torch.sigmoid(outputs)\n","            predicted_classes = (predicted_probs > 0.5).float()\n","\n","            correct_predictions += (predicted_classes == labels).sum().item()\n","            total_predictions += labels.size(0)\n","\n","        training_accuracy = correct_predictions / total_predictions\n","        epoch_loss = running_loss / len(train_dataloader)\n","        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss}, Accuracy: {training_accuracy * 100}%\")\n","\n","    return training_accuracy\n","\n","def evaluate(model, dataloader):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","\n","    model.eval()  # Set the model to evaluation mode\n","    all_labels = []\n","    all_predictions = []\n","\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            inputs = batch[0].to(device)\n","            labels = batch[1].to(device).float()\n","\n","            # Permute dimensions to (batch_size, input_size, sequence_length)\n","            inputs = inputs.permute(0, 2, 1)\n","\n","            outputs = model(inputs)\n","            predicted_probs = torch.sigmoid(outputs)\n","            predicted_classes = (predicted_probs > 0.5).float()\n","\n","            all_labels.extend(labels.cpu().numpy())\n","            all_predictions.extend(predicted_classes.cpu().numpy())\n","\n","    confusion = confusion_matrix(all_labels, all_predictions)\n","    return confusion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iCQKFBuFJVJn","tags":[],"executionInfo":{"status":"aborted","timestamp":1691116427800,"user_tz":240,"elapsed":43720,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"}}},"outputs":[],"source":["compare = SentimentGRU(embedding_dim, 128, 4, 0.8)  # Using 4 layers\n","train(compare, train_loader, 5e-05, 5, torch.optim.Adam(compare.parameters(), lr=5e-05))\n","\n","confusion_matrix = evaluate(compare, valid_loader)\n","print(\"Confusion Matrix:\")\n","print(confusion_matrix)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Qb_II8-JzIM"},"outputs":[],"source":["pip install hyperopt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1tB7hZnhJ0JO"},"outputs":[],"source":["from hyperopt import hp, fmin, tpe, Trials\n","\n","# Define the hyperparameter search space\n","space = {\n","    'lr': hp.choice('lr', [1e-5, 1e-4, 1e-3, 1e-2, 5e-5, 5e-4]),\n","    'hidden_size': hp.choice('hidden_size', [32, 64, 128, 256]),\n","    'num_layers': hp.choice('num_layers', [1, 2, 3]),\n","    'dropout': hp.choice('dropout', [0.0, 0.2,0.35, 0.5, 0.75,0.8,1]),  # Dropout rate\n","    'optimizer': hp.choice('optimizer', ['adam', 'sgd', 'rmsprop']),  # Optimizer choice\n","    'epochs': 2  # Run each model for 1 epoch\n","}\n","\n","# Initialize a list to store the top 10 hyperparameter configurations and their accuracies\n","top_configs = []\n","\n","# Define the objective function to optimize (in this case, the negative accuracy)\n","def objective(params):\n","    lr = params['lr']\n","    hidden_size = params['hidden_size']\n","    num_layers = params['num_layers']\n","    dropout = params['dropout']\n","    optimizer_choice = params['optimizer']\n","    epochs = params['epochs']\n","\n","    # Create the model with the given hyperparameters\n","    model = SentimentRNN(198, hidden_size, num_layers, dropout)\n","\n","    # Choose optimizer\n","    optimizer = None\n","    if optimizer_choice == 'adam':\n","        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    elif optimizer_choice == 'sgd':\n","        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n","    elif optimizer_choice == 'rmsprop':\n","        optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n","\n","    # Train the model and get the accuracy\n","    accuracy = train(model, train_loader, lr, epochs, optimizer)\n","\n","    # Add the hyperparameters and accuracy to the top_configs list\n","    top_configs.append({'lr': lr, 'hidden_size': hidden_size, 'num_layers': num_layers,\n","                        'dropout': dropout, 'optimizer': optimizer_choice,\n","                        'accuracy': accuracy})\n","\n","    # Since hyperopt minimizes the objective function, we need to return the negative accuracy\n","    return -accuracy\n","\n","# Initialize the Trials object to track the optimization process\n","trials = Trials()\n","\n","# Run the hyperparameter search using the Tree of Parzen Estimators (TPE) algorithm\n","best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=100, trials=trials)\n","\n","# Sort the top_configs list based on accuracy in descending order\n","top_configs.sort(key=lambda x: x['accuracy'], reverse=True)\n","\n","# Print the 10 best hyperparameter configurations and their accuracies\n","for i, config in enumerate(top_configs[:10]):\n","    print(f\"Top {i+1} Hyperparameters:\")\n","    print(f\"Learning Rate: {config['lr']}\")\n","    print(f\"Hidden Layer Size: {config['hidden_size']}\")\n","    print(f\"Number of Hidden Layers: {config['num_layers']}\")\n","    print(f\"Dropout Rate: {config['dropout']}\")\n","    print(f\"Optimizer: {config['optimizer']}\")\n","    print(f\"Accuracy: {config['accuracy']}\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WXSo7Ke6J4no"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","train_path = \"sentiment-emotion-labelled_Dell_tweets.csv\"\n","# use pathlib to load path\n","df = pd.read_csv(train_path, error_bad_lines=False)\n","\n","fig = plt.figure(figsize=(8,5))\n","ax = sns.barplot(x=df.sentiment.unique(),y=df.sentiment.value_counts());\n","ax.set(xlabel='Labels')\n","# Note: The label distribution is not even!"]}],"metadata":{"colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.17"}},"nbformat":4,"nbformat_minor":0}