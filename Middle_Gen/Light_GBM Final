{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNfFM6fIiIqTzZVnPYvJxAq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YbBoR1LDjeY4","executionInfo":{"status":"ok","timestamp":1689392075143,"user_tz":240,"elapsed":935613,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"}},"outputId":"d3cde576-eca9-4998-88c8-9091d3f2877f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy (Best Model): 0.6905405405405406\n","Best Parameters: {'max_depth': 20, 'n_estimators': 200, 'num_leaves': 30}\n","Accuracy (Initial Model): 0.6905405405405406\n","              precision    recall  f1-score   support\n","\n","           0       0.72      0.49      0.58      2696\n","           1       0.72      0.79      0.76      4380\n","           2       0.64      0.65      0.65      3605\n","           3       0.68      0.75      0.72      4119\n","\n","    accuracy                           0.69     14800\n","   macro avg       0.69      0.67      0.68     14800\n","weighted avg       0.69      0.69      0.69     14800\n","\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics import classification_report, accuracy_score\n","from lightgbm import LGBMClassifier\n","\n","# Read the CSV file into a DataFrame\n","tweet_data_int = pd.read_csv('/content/train_nick.csv')\n","\n","# Convert 'tweet' column to string data type and remove non-string entries\n","tweet_data_int['tweet'] = tweet_data_int['tweet'].astype(str)\n","tweet_data_int = tweet_data_int[tweet_data_int['tweet'].apply(lambda x: isinstance(x, str))]\n","\n","# Split the data into features (X) and target (y)\n","X = tweet_data_int['tweet']\n","y = tweet_data_int['sentiment']\n","\n","# Split the data into training, validation, and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n","\n","# Create TF-IDF vectorizer\n","vectorizer = TfidfVectorizer()\n","\n","# Fit the vectorizer on the training data and transform the training data\n","X_train_vectorized = vectorizer.fit_transform(X_train)\n","\n","# Transform the validation and testing data\n","X_val_vectorized = vectorizer.transform(X_val)\n","X_test_vectorized = vectorizer.transform(X_test)\n","\n","# Define the parameter grid for grid search\n","param_grid = {\n","    'num_leaves': [20, 30],\n","    'max_depth': [10, 20],\n","    'n_estimators': [100, 200]\n","}\n","\n","# Create the LightGBM classifier\n","lgbm = LGBMClassifier()\n","\n","# Perform grid search to find the best parameters\n","grid_search = GridSearchCV(lgbm, param_grid, cv=5, scoring='accuracy')\n","grid_search.fit(X_train_vectorized, y_train)\n","\n","# Get the best model and its parameters\n","best_model = grid_search.best_estimator_\n","best_params = grid_search.best_params_\n","\n","# Get predictions on the test set\n","y_pred = best_model.predict(X_test_vectorized)\n","\n","# Calculate accuracy for the best model\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","# Compare accuracy before and after tuning\n","y_pred_initial = best_model.predict(X_test_vectorized)\n","accuracy_initial = accuracy_score(y_test, y_pred_initial)\n","\n","# Print accuracy and best parameters\n","print(\"Accuracy (Best Model):\", accuracy)\n","print(\"Best Parameters:\", best_params)\n","print(\"Accuracy (Initial Model):\", accuracy_initial)\n","\n","# Print classification report\n","print(classification_report(y_test, y_pred))\n"]}]}