{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1343,"status":"ok","timestamp":1691127967808,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"},"user_tz":240},"id":"6mtHKphsGVRJ","outputId":"61feb06b-6d17-47c9-bc2d-f147c7432649"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4735,"status":"ok","timestamp":1691127972541,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"},"user_tz":240},"id":"iYJzPFoQGQoj","outputId":"5c281f5a-5d16-46f6-ab17-11db1e0b7e1e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (2.7.0)\n"]}],"source":["pip install emoji --upgrade"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9311,"status":"ok","timestamp":1691127981847,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"},"user_tz":240},"id":"BPtrjX9yGW50","outputId":"03fa13f8-b435-42d5-a73a-8883f730e7c3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.6)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.1)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"]}],"source":["!pip install nltk"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6630,"status":"ok","timestamp":1691127988464,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"},"user_tz":240},"id":"qbZESrVGGYg7","outputId":"1b3009ed-91d8-4d7c-f3c2-4253cf570948"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: contractions in /usr/local/lib/python3.10/dist-packages (0.1.73)\n","Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from contractions) (0.0.24)\n","Requirement already satisfied: anyascii in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n","Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n"]}],"source":["!pip install contractions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":345,"status":"ok","timestamp":1691127988804,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"},"user_tz":240},"id":"IXuI5rbGGZor","outputId":"4ec15b36-20ad-447c-ee23-22509ee5f788"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import pandas as pd\n","import re\n","import emoji\n","\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk.sentiment.util import mark_negation\n","import nltk\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","import contractions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1691127988804,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"},"user_tz":240},"id":"nZiKxYPmGa-j","outputId":"f5c14907-25fb-4d33-ecd0-dd0fcb3e1f57"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":142,"metadata":{},"output_type":"execute_result"}],"source":["nltk.download('wordnet')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":476,"status":"ok","timestamp":1691127989279,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"},"user_tz":240},"id":"2jBK6ciHGm1e","outputId":"2e1f0c90-cd0f-4e94-a9d2-01880e825467"},"outputs":[{"name":"stdout","output_type":"stream","text":["     id      company sentiment  \\\n","0  2401  Borderlands  Positive   \n","1  2401  Borderlands  Positive   \n","2  2401  Borderlands  Positive   \n","3  2401  Borderlands  Positive   \n","4  2401  Borderlands  Positive   \n","\n","                                               tweet  \n","0  im getting on borderlands and i will murder yo...  \n","1  I am coming to the borders and I will kill you...  \n","2  im getting on borderlands and i will kill you ...  \n","3  im coming on borderlands and i will murder you...  \n","4  im getting on borderlands 2 and i will murder ...  \n","     id    company   sentiment  \\\n","0  3364   Facebook  Irrelevant   \n","1   352     Amazon     Neutral   \n","2  8312  Microsoft    Negative   \n","3  4371      CS-GO    Negative   \n","4  4433     Google     Neutral   \n","\n","                                               tweet  \n","0  I mentioned on Facebook that I was struggling ...  \n","1  BBC News - Amazon boss Jeff Bezos rejects clai...  \n","2  @Microsoft Why do I pay for WORD when it funct...  \n","3  CSGO matchmaking is so full of closet hacking,...  \n","4  Now the President is slapping Americans in the...  \n"]}],"source":["# Loading the data\n","train_path = \"/content/drive/MyDrive/Colab Notebooks/project/twitter_sentiment/twitter_training.csv\"\n","val_path = \"/content/drive/MyDrive/Colab Notebooks/project/twitter_sentiment/twitter_validation.csv\"\n","# Note: ideally data doesnt have column headers but we add them to easy manipulation\n","headers = [\"id\", \"company\", \"sentiment\", \"tweet\"]\n","df_train = pd.read_csv(train_path, names=headers)\n","df_val = pd.read_csv(val_path, names=headers)\n","\n","print(df_train.head())\n","print(df_val.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HX2bpl4AGzhP"},"outputs":[],"source":["abbreviations = {\n","    \"$\" : \" dollar \",\n","    \"â‚¬\" : \" euro \",\n","    \"4ao\" : \"for adults only\",\n","    \"a.m\" : \"before midday\",\n","    \"a3\" : \"anytime anywhere anyplace\",\n","    \"aamof\" : \"as a matter of fact\",\n","    \"acct\" : \"account\",\n","    \"adih\" : \"another day in hell\",\n","    \"afaic\" : \"as far as i am concerned\",\n","    \"afaict\" : \"as far as i can tell\",\n","    \"afaik\" : \"as far as i know\",\n","    \"afair\" : \"as far as i remember\",\n","    \"afk\" : \"away from keyboard\",\n","    \"app\" : \"application\",\n","    \"approx\" : \"approximately\",\n","    \"apps\" : \"applications\",\n","    \"asap\" : \"as soon as possible\",\n","    \"asl\" : \"age, sex, location\",\n","    \"atk\" : \"at the keyboard\",\n","    \"ave.\" : \"avenue\",\n","    \"aymm\" : \"are you my mother\",\n","    \"ayor\" : \"at your own risk\",\n","    \"b&b\" : \"bed and breakfast\",\n","    \"b+b\" : \"bed and breakfast\",\n","    \"b.c\" : \"before christ\",\n","    \"b2b\" : \"business to business\",\n","    \"b2c\" : \"business to customer\",\n","    \"b4\" : \"before\",\n","    \"b4n\" : \"bye for now\",\n","    \"b@u\" : \"back at you\",\n","    \"bae\" : \"before anyone else\",\n","    \"bak\" : \"back at keyboard\",\n","    \"bbbg\" : \"bye bye be good\",\n","    \"bbc\" : \"british broadcasting corporation\",\n","    \"bbias\" : \"be back in a second\",\n","    \"bbl\" : \"be back later\",\n","    \"bbs\" : \"be back soon\",\n","    \"be4\" : \"before\",\n","    \"bfn\" : \"bye for now\",\n","    \"blvd\" : \"boulevard\",\n","    \"bout\" : \"about\",\n","    \"brb\" : \"be right back\",\n","    \"bros\" : \"brothers\",\n","    \"brt\" : \"be right there\",\n","    \"bsaaw\" : \"big smile and a wink\",\n","    \"btw\" : \"by the way\",\n","    \"bwl\" : \"bursting with laughter\",\n","    \"c/o\" : \"care of\",\n","    \"cet\" : \"central european time\",\n","    \"cf\" : \"compare\",\n","    \"cia\" : \"central intelligence agency\",\n","    \"csl\" : \"can not stop laughing\",\n","    \"cu\" : \"see you\",\n","    \"cul8r\" : \"see you later\",\n","    \"cv\" : \"curriculum vitae\",\n","    \"cwot\" : \"complete waste of time\",\n","    \"cya\" : \"see you\",\n","    \"cyt\" : \"see you tomorrow\",\n","    \"dae\" : \"does anyone else\",\n","    \"dbmib\" : \"do not bother me i am busy\",\n","    \"diy\" : \"do it yourself\",\n","    \"dm\" : \"direct message\",\n","    \"dwh\" : \"during work hours\",\n","    \"e123\" : \"easy as one two three\",\n","    \"eet\" : \"eastern european time\",\n","    \"eg\" : \"example\",\n","    \"embm\" : \"early morning business meeting\",\n","    \"encl\" : \"enclosed\",\n","    \"encl.\" : \"enclosed\",\n","    \"etc\" : \"and so on\",\n","    \"faq\" : \"frequently asked questions\",\n","    \"fawc\" : \"for anyone who cares\",\n","    \"fb\" : \"facebook\",\n","    \"fc\" : \"fingers crossed\",\n","    \"fig\" : \"figure\",\n","    \"fimh\" : \"forever in my heart\",\n","    \"ft.\" : \"feet\",\n","    \"ft\" : \"featuring\",\n","    \"ftl\" : \"for the loss\",\n","    \"ftw\" : \"for the win\",\n","    \"fwiw\" : \"for what it is worth\",\n","    \"fyi\" : \"for your information\",\n","    \"g9\" : \"genius\",\n","    \"gahoy\" : \"get a hold of yourself\",\n","    \"gal\" : \"get a life\",\n","    \"gcse\" : \"general certificate of secondary education\",\n","    \"gfn\" : \"gone for now\",\n","    \"gg\" : \"good game\",\n","    \"gl\" : \"good luck\",\n","    \"glhf\" : \"good luck have fun\",\n","    \"gmt\" : \"greenwich mean time\",\n","    \"gmta\" : \"great minds think alike\",\n","    \"gn\" : \"good night\",\n","    \"g.o.a.t\" : \"greatest of all time\",\n","    \"goat\" : \"greatest of all time\",\n","    \"goi\" : \"get over it\",\n","    \"gps\" : \"global positioning system\",\n","    \"gr8\" : \"great\",\n","    \"gratz\" : \"congratulations\",\n","    \"gyal\" : \"girl\",\n","    \"h&c\" : \"hot and cold\",\n","    \"hp\" : \"horsepower\",\n","    \"hr\" : \"hour\",\n","    \"hrh\" : \"his royal highness\",\n","    \"ht\" : \"height\",\n","    \"ibrb\" : \"i will be right back\",\n","    \"ic\" : \"i see\",\n","    \"icq\" : \"i seek you\",\n","    \"icymi\" : \"in case you missed it\",\n","    \"idc\" : \"i do not care\",\n","    \"idgadf\" : \"i do not give a damn fuck\",\n","    \"idgaf\" : \"i do not give a fuck\",\n","    \"idk\" : \"i do not know\",\n","    \"ie\" : \"that is\",\n","    \"i.e\" : \"that is\",\n","    \"ifyp\" : \"i feel your pain\",\n","    \"IG\" : \"instagram\",\n","    \"iirc\" : \"if i remember correctly\",\n","    \"ilu\" : \"i love you\",\n","    \"ily\" : \"i love you\",\n","    \"imho\" : \"in my humble opinion\",\n","    \"imo\" : \"in my opinion\",\n","    \"imu\" : \"i miss you\",\n","    \"iow\" : \"in other words\",\n","    \"irl\" : \"in real life\",\n","    \"j4f\" : \"just for fun\",\n","    \"jic\" : \"just in case\",\n","    \"jk\" : \"just kidding\",\n","    \"jsyk\" : \"just so you know\",\n","    \"l8r\" : \"later\",\n","    \"lb\" : \"pound\",\n","    \"lbs\" : \"pounds\",\n","    \"ldr\" : \"long distance relationship\",\n","    \"lmao\" : \"laugh my ass off\",\n","    \"lmfao\" : \"laugh my fucking ass off\",\n","    \"lol\" : \"laughing out loud\",\n","    \"ltd\" : \"limited\",\n","    \"ltns\" : \"long time no see\",\n","    \"m8\" : \"mate\",\n","    \"mf\" : \"motherfucker\",\n","    \"mfs\" : \"motherfuckers\",\n","    \"mfw\" : \"my face when\",\n","    \"mofo\" : \"motherfucker\",\n","    \"mph\" : \"miles per hour\",\n","    \"mr\" : \"mister\",\n","    \"mrw\" : \"my reaction when\",\n","    \"ms\" : \"miss\",\n","    \"mte\" : \"my thoughts exactly\",\n","    \"nagi\" : \"not a good idea\",\n","    \"nbc\" : \"national broadcasting company\",\n","    \"nbd\" : \"not big deal\",\n","    \"nfs\" : \"not for sale\",\n","    \"ngl\" : \"not going to lie\",\n","    \"nhs\" : \"national health service\",\n","    \"nrn\" : \"no reply necessary\",\n","    \"nsfl\" : \"not safe for life\",\n","    \"nsfw\" : \"not safe for work\",\n","    \"nth\" : \"nice to have\",\n","    \"nvr\" : \"never\",\n","    \"nyc\" : \"new york city\",\n","    \"oc\" : \"original content\",\n","    \"og\" : \"original\",\n","    \"ohp\" : \"overhead projector\",\n","    \"oic\" : \"oh i see\",\n","    \"omdb\" : \"over my dead body\",\n","    \"omg\" : \"oh my god\",\n","    \"omw\" : \"on my way\",\n","    \"p.a\" : \"per annum\",\n","    \"p.m\" : \"after midday\",\n","    \"pm\" : \"prime minister\",\n","    \"poc\" : \"people of color\",\n","    \"pov\" : \"point of view\",\n","    \"pp\" : \"pages\",\n","    \"ppl\" : \"people\",\n","    \"prw\" : \"parents are watching\",\n","    \"ps\" : \"postscript\",\n","    \"pt\" : \"point\",\n","    \"ptb\" : \"please text back\",\n","    \"pto\" : \"please turn over\",\n","    \"qpsa\" : \"what happens\", #\"que pasa\",\n","    \"ratchet\" : \"rude\",\n","    \"rbtl\" : \"read between the lines\",\n","    \"rlrt\" : \"real life retweet\",\n","    \"rofl\" : \"rolling on the floor laughing\",\n","    \"roflol\" : \"rolling on the floor laughing out loud\",\n","    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n","    \"rt\" : \"retweet\",\n","    \"ruok\" : \"are you ok\",\n","    \"sfw\" : \"safe for work\",\n","    \"sk8\" : \"skate\",\n","    \"smh\" : \"shake my head\",\n","    \"sq\" : \"square\",\n","    \"srsly\" : \"seriously\",\n","    \"ssdd\" : \"same stuff different day\",\n","    \"tbh\" : \"to be honest\",\n","    \"tbs\" : \"tablespooful\",\n","    \"tbsp\" : \"tablespooful\",\n","    \"tfw\" : \"that feeling when\",\n","    \"thks\" : \"thank you\",\n","    \"tho\" : \"though\",\n","    \"thx\" : \"thank you\",\n","    \"tia\" : \"thanks in advance\",\n","    \"til\" : \"today i learned\",\n","    \"tl;dr\" : \"too long i did not read\",\n","    \"tldr\" : \"too long i did not read\",\n","    \"tmb\" : \"tweet me back\",\n","    \"tntl\" : \"trying not to laugh\",\n","    \"ttyl\" : \"talk to you later\",\n","    \"u\" : \"you\",\n","    \"u2\" : \"you too\",\n","    \"u4e\" : \"yours for ever\",\n","    \"utc\" : \"coordinated universal time\",\n","    \"w/\" : \"with\",\n","    \"w/o\" : \"without\",\n","    \"w8\" : \"wait\",\n","    \"wassup\" : \"what is up\",\n","    \"wb\" : \"welcome back\",\n","    \"wtf\" : \"what the fuck\",\n","    \"wtg\" : \"way to go\",\n","    \"wtpa\" : \"where the party at\",\n","    \"wuf\" : \"where are you from\",\n","    \"wuzup\" : \"what is up\",\n","    \"wywh\" : \"wish you were here\",\n","    \"yd\" : \"yard\",\n","    \"ygtr\" : \"you got that right\",\n","    \"ynk\" : \"you never know\",\n","    \"zzz\" : \"sleeping bored and tired\"\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H9qMwKl5G1Is"},"outputs":[],"source":["# Step 1: Remove NaN values\n","def remove_nan_values(df, columns):\n","    for column in columns:\n","        df = df[df[column].notna()]  # Remove rows with NaN values\n","        df = df[df[column].astype(bool)]  # Remove rows with empty lists or lists with only white spaces\n","    return df\n","\n","# Step 2: Assign tweet labels\n","def one_hot_encode_sentiments(sentiment): #make neutral and irrelavent same number, and look over\n","    if sentiment == 'Positive':\n","        return 2\n","    elif sentiment == 'Negative':\n","        return 0\n","    elif sentiment == 'Neutral':\n","        return 1\n","    elif sentiment == 'Irrelevant':\n","        return 1\n","\n","def expand_slang(text):\n","    tokens = word_tokenize(text)\n","    tokens = [abbreviations.get(word.lower(), word) for word in tokens]\n","    text = ' '.join(tokens)\n","    return text\n","\n","\n","# Step 3: Clean tweets\n","def clean_tweets(tweet):\n","    # Remove @mentions\n","    tweet = re.sub(r'@\\w+', '', tweet)\n","\n","    # Remove URLs\n","    tweet = re.sub(r'http\\S+|www\\S+', '', tweet)\n","\n","    # Remove hashtags\n","    tweet = re.sub(r'#\\w+', '', tweet)\n","\n","    # Convert emojis to text\n","    tweet = emoji.demojize(tweet)\n","\n","    # Expand contractions\n","    tweet = contractions.fix(tweet)\n","    ##############################3\n","\n","    # Remove numbers\n","    tweet = re.sub(r'\\d+', '', tweet)\n","\n","    # Remove punctuation\n","    tweet = re.sub(r'[^\\w\\s]', '', tweet)\n","\n","    # Remove excessive whitespace\n","    tweet = re.sub(r'\\s+', ' ', tweet)\n","\n","    # Standardize repeated characters\n","    tweet = re.sub(r'(.)\\1+', r'\\1\\1', tweet)\n","\n","    #Expand slang terms\n","    tweet = expand_slang(tweet)\n","\n","    # Tokenize the tweet\n","    tokens = word_tokenize(tweet)\n","\n","    # Lowercase and normalize\n","    tokens = [token.lower() for token in tokens]\n","\n","    # Remove stop words\n","    stop_words = set(stopwords.words('english'))\n","    tokens = [token for token in tokens if token.lower() not in stop_words]\n","\n","    # Lemmatization good neg\n","    lemmatizer = WordNetLemmatizer()\n","    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n","\n","    # Handle negation using mark_negation\n","    #tokens = mark_negation(tokens)\n","    #tweet = \"I don't like this product. It is not good.\"\n","    #['I', \"don't\", 'like_NEG', 'this_NEG', 'product_NEG', '.', 'It_NEG', 'is_NEG', 'not_NEG', 'good_NEG', '.']\n","\n","    # Uncomment the following lines to include POS tags as comments\n","    #pos_tags = nltk.pos_tag(tokens)\n","    #tokens_with_pos = [f\"{token}/{pos}\" for token, pos in pos_tags]\n","\n","    return tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1691127989279,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"},"user_tz":240},"id":"A3xbRrvFCxvs","outputId":"12d42cf8-4c53-4991-eac4-e9c4a824ed9c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Data Sentiment Counts:\n","Positive (2): 18318\n","Neutral (1): 20832\n","Negative (0): 22542\n","\n","Validation Data Sentiment Counts:\n","Positive (2): 266\n","Neutral (1): 277\n","Negative (0): 285\n"]}],"source":["# Print the counts of positive, neutral, and negative sentiments\n","train_counts = df_train['sentiment'].value_counts()\n","val_counts = df_val['sentiment'].value_counts()\n","\n","print(\"Training Data Sentiment Counts:\")\n","print(f\"Positive (2): {train_counts.get(2, 0)}\")\n","print(f\"Neutral (1): {train_counts.get(1, 0)}\")\n","print(f\"Negative (0): {train_counts.get(0, 0)}\")\n","\n","print(\"\\nValidation Data Sentiment Counts:\")\n","print(f\"Positive (2): {val_counts.get(2, 0)}\")\n","print(f\"Neutral (1): {val_counts.get(1, 0)}\")\n","print(f\"Negative (0): {val_counts.get(0, 0)}\")\n","\n","# Rest of the code"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1691127989280,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"},"user_tz":240},"id":"o4XLR8RRG7PD","outputId":"e8aafd5d-a146-4b21-ce39-8340dca6bcf6"},"outputs":[{"name":"stdout","output_type":"stream","text":["     id      company sentiment  \\\n","0  2401  Borderlands  Positive   \n","1  2401  Borderlands  Positive   \n","2  2401  Borderlands  Positive   \n","3  2401  Borderlands  Positive   \n","4  2401  Borderlands  Positive   \n","\n","                                               tweet  \n","0  im getting on borderlands and i will murder yo...  \n","1  I am coming to the borders and I will kill you...  \n","2  im getting on borderlands and i will kill you ...  \n","3  im coming on borderlands and i will murder you...  \n","4  im getting on borderlands 2 and i will murder ...  \n","     id    company   sentiment  \\\n","0  3364   Facebook  Irrelevant   \n","1   352     Amazon     Neutral   \n","2  8312  Microsoft    Negative   \n","3  4371      CS-GO    Negative   \n","4  4433     Google     Neutral   \n","\n","                                               tweet  \n","0  I mentioned on Facebook that I was struggling ...  \n","1  BBC News - Amazon boss Jeff Bezos rejects clai...  \n","2  @Microsoft Why do I pay for WORD when it funct...  \n","3  CSGO matchmaking is so full of closet hacking,...  \n","4  Now the President is slapping Americans in the...  \n"]}],"source":["# Step 1: Remove NaN values\n","columns_to_check = [\"id\", \"company\", \"sentiment\", \"tweet\"]\n","df_train = remove_nan_values(df_train, columns_to_check)\n","df_val = remove_nan_values(df_val, columns_to_check)\n","\n","print(df_train.head())\n","print(df_val.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":802,"status":"ok","timestamp":1691127990079,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"},"user_tz":240},"id":"obq9v6IMG8i9","outputId":"cfd4b777-0b79-404f-8a9d-b8bc631e6959"},"outputs":[{"name":"stdout","output_type":"stream","text":["     id      company  sentiment  \\\n","0  2401  Borderlands          2   \n","1  2401  Borderlands          2   \n","2  2401  Borderlands          2   \n","3  2401  Borderlands          2   \n","4  2401  Borderlands          2   \n","\n","                                               tweet  \n","0  im getting on borderlands and i will murder yo...  \n","1  I am coming to the borders and I will kill you...  \n","2  im getting on borderlands and i will kill you ...  \n","3  im coming on borderlands and i will murder you...  \n","4  im getting on borderlands 2 and i will murder ...  \n","     id    company  sentiment  \\\n","0  3364   Facebook          1   \n","1   352     Amazon          1   \n","2  8312  Microsoft          0   \n","3  4371      CS-GO          0   \n","4  4433     Google          1   \n","\n","                                               tweet  \n","0  I mentioned on Facebook that I was struggling ...  \n","1  BBC News - Amazon boss Jeff Bezos rejects clai...  \n","2  @Microsoft Why do I pay for WORD when it funct...  \n","3  CSGO matchmaking is so full of closet hacking,...  \n","4  Now the President is slapping Americans in the...  \n"]}],"source":["df_train['sentiment'] = df_train['sentiment'].apply(one_hot_encode_sentiments)\n","df_val['sentiment'] = df_val['sentiment'].apply(one_hot_encode_sentiments)\n","\n","print(df_train.head())\n","print(df_val.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1691127990079,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"},"user_tz":240},"id":"WEEYHSI3C6ha","outputId":"a9dc13c0-31cb-4ba9-8676-73b0df9f0ffa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Data Sentiment Counts:\n","Positive (2): 20655\n","Neutral (1): 30983\n","Negative (0): 22358\n","\n","Validation Data Sentiment Counts:\n","Positive (2): 277\n","Neutral (1): 457\n","Negative (0): 266\n"]}],"source":["# Print the counts of positive, neutral, and negative sentiments\n","train_counts = df_train['sentiment'].value_counts()\n","val_counts = df_val['sentiment'].value_counts()\n","\n","print(\"Training Data Sentiment Counts:\")\n","print(f\"Positive (2): {train_counts.get(2, 0)}\")\n","print(f\"Neutral (1): {train_counts.get(1, 0)}\")\n","print(f\"Negative (0): {train_counts.get(0, 0)}\")\n","\n","print(\"\\nValidation Data Sentiment Counts:\")\n","print(f\"Positive (2): {val_counts.get(2, 0)}\")\n","print(f\"Neutral (1): {val_counts.get(1, 0)}\")\n","print(f\"Negative (0): {val_counts.get(0, 0)}\")\n","\n","# Rest of the code"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":81081,"status":"ok","timestamp":1691128071158,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"},"user_tz":240},"id":"W_5aw4nVG91C","outputId":"d87fea45-ff63-403a-dc5c-c6ddac74190e"},"outputs":[{"name":"stdout","output_type":"stream","text":["     id      company  sentiment                          tweet\n","0  2401  Borderlands          2  [getting, borderland, murder]\n","1  2401  Borderlands          2         [coming, border, kill]\n","2  2401  Borderlands          2    [getting, borderland, kill]\n","3  2401  Borderlands          2   [coming, borderland, murder]\n","4  2401  Borderlands          2  [getting, borderland, murder]\n","     id    company  sentiment  \\\n","0  3364   Facebook          1   \n","1   352     Amazon          1   \n","2  8312  Microsoft          0   \n","3  4371      CS-GO          0   \n","4  4433     Google          1   \n","\n","                                               tweet  \n","0  [mentioned, facebook, struggling, motivation, ...  \n","1  [british, broadcasting, corporation, news, ama...  \n","2  [pay, word, function, poorly, chromebook, face...  \n","3  [csgo, matchmaking, full, closet, hacking, tru...  \n","4  [president, slapping, american, face, really, ...  \n"]}],"source":["# Step 3: Clean tweets\n","df_train['tweet'] = df_train['tweet'].apply(clean_tweets)\n","df_val['tweet'] = df_val['tweet'].apply(clean_tweets)\n","\n","print(df_train.head())\n","print(df_val.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1691128071159,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"},"user_tz":240},"id":"9Q1QjG14DSs7","outputId":"3ffec4af-e66f-4a35-d29f-2c1d4e094635"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Data Sentiment Counts:\n","Positive (2): 20655\n","Neutral (1): 30983\n","Negative (0): 22358\n","\n","Validation Data Sentiment Counts:\n","Positive (2): 277\n","Neutral (1): 457\n","Negative (0): 266\n"]}],"source":["# Print the counts of positive, neutral, and negative sentiments\n","train_counts = df_train['sentiment'].value_counts()\n","val_counts = df_val['sentiment'].value_counts()\n","\n","print(\"Training Data Sentiment Counts:\")\n","print(f\"Positive (2): {train_counts.get(2, 0)}\")\n","print(f\"Neutral (1): {train_counts.get(1, 0)}\")\n","print(f\"Negative (0): {train_counts.get(0, 0)}\")\n","\n","print(\"\\nValidation Data Sentiment Counts:\")\n","print(f\"Positive (2): {val_counts.get(2, 0)}\")\n","print(f\"Neutral (1): {val_counts.get(1, 0)}\")\n","print(f\"Negative (0): {val_counts.get(0, 0)}\")\n","\n","# Rest of the code"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1691128071159,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"},"user_tz":240},"id":"zD3zM14xHAlZ","outputId":"09ade2c6-0c16-481d-dd05-cf9be0897b99"},"outputs":[{"name":"stdout","output_type":"stream","text":["     id      company  sentiment                          tweet\n","0  2401  Borderlands          2  [getting, borderland, murder]\n","1  2401  Borderlands          2         [coming, border, kill]\n","2  2401  Borderlands          2    [getting, borderland, kill]\n","3  2401  Borderlands          2   [coming, borderland, murder]\n","4  2401  Borderlands          2  [getting, borderland, murder]\n","     id    company  sentiment  \\\n","0  3364   Facebook          1   \n","1   352     Amazon          1   \n","2  8312  Microsoft          0   \n","3  4371      CS-GO          0   \n","4  4433     Google          1   \n","\n","                                               tweet  \n","0  [mentioned, facebook, struggling, motivation, ...  \n","1  [british, broadcasting, corporation, news, ama...  \n","2  [pay, word, function, poorly, chromebook, face...  \n","3  [csgo, matchmaking, full, closet, hacking, tru...  \n","4  [president, slapping, american, face, really, ...  \n"]}],"source":["# Print the resulting DataFrame\n","print(df_train.head())\n","print(df_val.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OqMQesJiHCzv"},"outputs":[],"source":["# Assuming your DataFrame is named df\n","import torch\n","# Drop the 'id' column\n","df_train_new = df_train.drop('id', axis=1)\n","df_val_new = df_val.drop('id', axis=1)\n","\n","# Export the DataFrame to a CSV file\n","df_train_new.to_csv('df_train_new_int.csv', index=False)\n","df_val_new.to_csv('df_val_new_int.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1691128071847,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"},"user_tz":240},"id":"NojZ5_e5HEhs","outputId":"a5efe6df-6d6b-4058-e5b9-5179bbe7f468"},"outputs":[{"name":"stdout","output_type":"stream","text":["       company  sentiment                          tweet\n","0  Borderlands          2  [getting, borderland, murder]\n","1  Borderlands          2         [coming, border, kill]\n","2  Borderlands          2    [getting, borderland, kill]\n","3  Borderlands          2   [coming, borderland, murder]\n","4  Borderlands          2  [getting, borderland, murder]\n"]}],"source":["print(df_train_new[:5])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ThMFklLIySkp"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from nltk.tokenize import word_tokenize\n","from collections import Counter\n","import pandas as pd\n","import re\n","import emoji\n","import contractions\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4MuaieNNyVXK"},"outputs":[],"source":["# Load your preprocessed data\n","series_of_lists_train = df_train_new['tweet']\n","series_of_lists_valid = df_val_new['tweet']\n","\n","# Create a vocabulary and a word-to-index dictionary\n","all_words = [word for tweet in series_of_lists_train for word in tweet]\n","word_counts = Counter(all_words)\n","vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n","word_to_index = {word: idx for idx, word in enumerate(vocab)}\n","\n","# Define the maximum sequence length\n","MAX_SEQUENCE_LENGTH = 198"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JzVWTXZNyeb6"},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, data, labels, word_to_index):\n","        self.data = data\n","        self.labels = labels\n","        self.word_to_index = word_to_index\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        tokens = self.data.iloc[idx]\n","        indices = [self.word_to_index[word] for word in tokens if word in self.word_to_index]\n","        padded_indices = indices + [0] * (MAX_SEQUENCE_LENGTH - len(indices))\n","        return torch.tensor(padded_indices), torch.tensor(self.labels.iloc[idx])\n","\n","# Create Custom Datasets and DataLoaders\n","train_dataset = CustomDataset(series_of_lists_train, df_train['sentiment'], word_to_index)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","\n","val_dataset = CustomDataset(series_of_lists_valid, df_val['sentiment'], word_to_index)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jbTEs5FMvMsm"},"outputs":[],"source":["# Define your CNN architecture\n","class CNNModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, num_classes, dropout_percent=0.5):\n","        super(CNNModel, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.conv1 = nn.Conv1d(embedding_dim, 128, kernel_size=3)\n","        self.conv2 = nn.Conv1d(128, 64, kernel_size=3)\n","        self.pool = nn.MaxPool1d(3)  # Change the pooling size\n","        self.fc1 = nn.Linear(64 * 21, 128)  # Adjust the linear layer input and output sizes\n","        self.fc2 = nn.Linear(128, num_classes)\n","        self.dropout = nn.Dropout(dropout_percent)\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        x = x.permute(0, 2, 1)\n","        x = F.relu(self.conv1(x))\n","        x = self.pool(x)\n","        x = F.relu(self.conv2(x))\n","        x = self.pool(x)\n","        x = x.view(x.size(0), -1)\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","        return x\n","\n","def train(model, train_dataloader, learning_rate, num_epochs, optimizer):\n","  # Training loop\n","  criterion = nn.CrossEntropyLoss()\n","\n","  for epoch in range(num_epochs):\n","      model.train()\n","      for inputs, labels in train_loader:\n","          optimizer.zero_grad()\n","          outputs = model(inputs)\n","          loss = criterion(outputs, labels)\n","          loss.backward()\n","          optimizer.step()\n","\n","      model.eval()\n","      val_loss = 0.0\n","      correct = 0\n","      total = 0\n","      with torch.no_grad():\n","          for inputs, labels in val_loader:\n","              outputs = model(inputs)\n","              val_loss += criterion(outputs, labels).item()\n","              _, predicted = torch.max(outputs, 1)\n","              total += labels.size(0)\n","              correct += (predicted == labels).sum().item()\n","\n","              # Print labels and predictions for debugging\n","              #print(\"Labels:\", labels)\n","              #print(\"Predictions:\", predicted)\n","\n","      print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n","            f\"Validation Loss: {val_loss / len(val_loader):.4f} - \"\n","            f\"Validation Accuracy: {(correct / total) * 100:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"elapsed":29464,"status":"error","timestamp":1691131068705,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"},"user_tz":240},"id":"TA7uJPhyyhy6","outputId":"7b39da5e-018e-4cc9-82b2-f4e94c3cd9e4"},"outputs":[{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-197-adca674a955f>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#criterion = nn.CrossEntropyLoss()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-196-c1c51919f2e0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, learning_rate, num_epochs, optimizer)\u001b[0m\n\u001b[1;32m     34\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m           \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Initialize the model\n","vocab_size = len(vocab)\n","embedding_dim = 100\n","num_classes = 3\n","learning_rate = 0.001\n","num_epochs = 10\n","dropout_percent = 0.5\n","kernel_num = 3\n","\n","model = CNNModel(vocab_size, embedding_dim, num_classes, dropout_percent)\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Define loss function and optimizer\n","#criterion = nn.CrossEntropyLoss()\n","\n","train(model, train_loader, learning_rate, num_epochs, optimizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YSeaKAfN8rMc"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","\n","# Evaluate the model\n","def evaluate_model(model, data_loader):\n","    model.eval()\n","    all_labels = []\n","    all_predictions = []\n","    with torch.no_grad():\n","        for inputs, labels in data_loader:\n","            outputs = model(inputs)\n","            _, predicted = torch.max(outputs, 1)\n","            all_labels.extend(labels.tolist())\n","            all_predictions.extend(predicted.tolist())\n","    return all_labels, all_predictions\n","\n","# Evaluate the model on the validation set\n","val_labels, val_predictions = evaluate_model(model, val_loader)\n","\n","# Compute the confusion matrix\n","cm = confusion_matrix(val_labels, val_predictions, labels=[0, 1, 2])\n","\n","# Plot the confusion matrix\n","classes = ['Negative', 'Neutral', 'Positive']\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n","disp.plot(cmap=plt.cm.Blues, values_format='.0f')\n","plt.title(\"Confusion Matrix\")\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WGZsAhqo3dvm"},"outputs":[],"source":["confusion_matrix = evaluate_model(model, val_loader)\n","print(\"Confusion Matrix:\")\n","print(confusion_matrix)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YZOcUBiq-Xfz"},"outputs":[],"source":["# Run the Model on New Data\n","def predict_sentiment(model, text, word_to_index):\n","    tokens = word_tokenize(text)\n","    indices = [word_to_index[word] for word in tokens if word in word_to_index]\n","    padded_indices = indices + [0] * (MAX_SEQUENCE_LENGTH - len(indices))\n","    inputs = torch.tensor(padded_indices).unsqueeze(0)\n","    outputs = model(inputs)\n","    _, predicted = torch.max(outputs, 1)\n","    predicted_label = predicted.item()\n","\n","    # Get the probability scores for each class\n","    softmax_scores = torch.nn.functional.softmax(outputs, dim=1)\n","    negative_prob = softmax_scores[0][0].item()  # Probability of negative class\n","    neutral_prob = softmax_scores[0][1].item()   # Probability of neutral class\n","    positive_prob = softmax_scores[0][2].item()  # Probability of positive class\n","\n","    return predicted_label, negative_prob, neutral_prob, positive_prob\n","\n","new_text = \"poorly, chromebook, facebook\"\n","predicted_label, negative_prob, neutral_prob, positive_prob = predict_sentiment(model, new_text, word_to_index)\n","\n","print(f\"Predicted Label: {predicted_label}\")\n","print(f\"Negative Probability: {negative_prob}\")\n","print(f\"Neutral Probability: {neutral_prob}\")\n","print(f\"Positive Probability: {positive_prob}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9144,"status":"ok","timestamp":1691130626640,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"},"user_tz":240},"id":"JybAmAFvJJBc","outputId":"31bc1f32-8350-4478-c25d-1b81aa8b9ded"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: hyperopt in /usr/local/lib/python3.10/dist-packages (0.2.7)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from hyperopt) (1.22.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from hyperopt) (1.10.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from hyperopt) (1.16.0)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from hyperopt) (3.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from hyperopt) (0.18.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from hyperopt) (4.65.0)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from hyperopt) (2.2.1)\n","Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (from hyperopt) (0.10.9.7)\n"]}],"source":["pip install hyperopt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"kSFBjGW4B0pC","outputId":"7c3b1a98-02d7-4175-99ea-310cd98a9045"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/2] - Validation Loss: 1.0945 - Validation Accuracy: 38.90%\n","Epoch [2/2] - Validation Loss: 1.0920 - Validation Accuracy: 45.60%\n","  0%|          | 0/100 [07:55<?, ?trial/s, best loss=?]"]},{"name":"stderr","output_type":"stream","text":["ERROR:hyperopt.fmin:job exception: bad operand type for unary -: 'NoneType'\n"]},{"name":"stdout","output_type":"stream","text":["\r  0%|          | 0/100 [07:55<?, ?trial/s, best loss=?]\n"]},{"ename":"TypeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-200-06d21072d8ca>\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# Run the hyperparameter search using the Tree of Parzen Estimators (TPE) algorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Sort the top_configs list based on accuracy in descending order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mallow_trials_fmin\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fmin\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m         return trials.fmin(\n\u001b[0m\u001b[1;32m    541\u001b[0m             \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfmin\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         return fmin(\n\u001b[0m\u001b[1;32m    672\u001b[0m             \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;31m# next line is where the fmin is actually executed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                     \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"job exception: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    890\u001b[0m                 \u001b[0mprint_node_on_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrec_eval_print_node_on_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             )\n\u001b[0;32m--> 892\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-200-06d21072d8ca>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# Since hyperopt minimizes the objective function, we need to return the negative accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# Initialize the Trials object to track the optimization process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: bad operand type for unary -: 'NoneType'"]}],"source":["from hyperopt import hp, fmin, tpe, Trials\n","\n","# Define the hyperparameter search space\n","space = {\n","    'lr': hp.choice('lr', [1e-5, 1e-4, 1e-3, 1e-2, 5e-5, 5e-4]),\n","    'dropout': hp.choice('dropout', [0.0, 0.2, 0.35, 0.5, 0.75, 0.8, 1]),  # Dropout rate\n","    'optimizer': hp.choice('optimizer', ['adam', 'sgd', 'rmsprop']),  # Optimizer choice\n","    'epochs': 2  # Run each model for 1 epoch\n","}\n","\n","# Initialize a list to store the top 10 hyperparameter configurations and their accuracies\n","top_configs = []\n","\n","# Define the objective function to optimize (in this case, the negative accuracy)\n","def objective(params):\n","    vocab_size = len(vocab)\n","    embedding_dim = 100\n","    num_classes = 3\n","\n","    lr = params['lr']\n","    dropout = params['dropout']\n","    optimizer_choice = params['optimizer']\n","    epochs = params['epochs']\n","\n","    # Create the model with the given hyperparameters\n","    model = CNNModel(vocab_size, embedding_dim, num_classes, dropout)\n","\n","    # Choose optimizer\n","    optimizer = None\n","    if optimizer_choice == 'adam':\n","        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    elif optimizer_choice == 'sgd':\n","        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n","    elif optimizer_choice == 'rmsprop':\n","        optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n","\n","    # Train the model and get the accuracy\n","    accuracy = train(model, train_loader, lr, epochs, optimizer)\n","\n","    # Add the hyperparameters and accuracy to the top_configs list\n","    top_configs.append({'lr': lr, 'dropout': dropout, 'optimizer': optimizer_choice, 'accuracy': accuracy})\n","\n","    # Since hyperopt minimizes the objective function, we need to return the negative accuracy\n","    return -accuracy\n","\n","# Initialize the Trials object to track the optimization process\n","trials = Trials()\n","\n","# Run the hyperparameter search using the Tree of Parzen Estimators (TPE) algorithm\n","best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=100, trials=trials)\n","\n","# Sort the top_configs list based on accuracy in descending order\n","top_configs.sort(key=lambda x: x['accuracy'], reverse=True)\n","\n","# Print the 10 best hyperparameter configurations and their accuracies\n","for i, config in enumerate(top_configs[:10]):\n","    print(f\"Top {i+1} Hyperparameters:\")\n","    print(f\"Learning Rate: {config['lr']}\")\n","    print(f\"Dropout Rate: {config['dropout']}\")\n","    print(f\"Optimizer: {config['optimizer']}\")\n","    print(f\"Accuracy: {config['accuracy']}\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bsxwhdg8tP0J","outputId":"aad85fb3-0969-46b6-bbbf-4559444389d4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/10] - Validation Loss: 0.2733 - Validation Accuracy: 90.19%\n","Epoch [2/10] - Validation Loss: 0.1336 - Validation Accuracy: 96.05%\n","Epoch [3/10] - Validation Loss: 0.1122 - Validation Accuracy: 96.32%\n","Epoch [4/10] - Validation Loss: 0.0963 - Validation Accuracy: 97.14%\n","Epoch [5/10] - Validation Loss: 0.1149 - Validation Accuracy: 97.68%\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from nltk.tokenize import word_tokenize\n","from collections import Counter\n","import pandas as pd\n","import re\n","import emoji\n","import contractions\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","# Load your preprocessed data\n","series_of_lists_train = df_train_new['tweet']\n","series_of_lists_valid = df_val_new['tweet']\n","\n","# Create a vocabulary and a word-to-index dictionary\n","all_words = [word for tweet in series_of_lists_train for word in tweet]\n","word_counts = Counter(all_words)\n","vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n","word_to_index = {word: idx for idx, word in enumerate(vocab)}\n","\n","# Define the maximum sequence length\n","MAX_SEQUENCE_LENGTH = 198\n","\n","# Define your CNN architecture\n","class CNNModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, num_classes):\n","        super(CNNModel, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.conv1 = nn.Conv1d(embedding_dim, 128, kernel_size=3)\n","        self.conv2 = nn.Conv1d(128, 64, kernel_size=3)\n","        self.pool = nn.MaxPool1d(3)  # Change the pooling size\n","        self.fc1 = nn.Linear(64 * 21, 128)  # Adjust the linear layer input and output sizes\n","        self.fc2 = nn.Linear(128, num_classes)\n","        self.dropout = nn.Dropout(0.5)\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        x = x.permute(0, 2, 1)\n","        x = F.relu(self.conv1(x))\n","        x = self.pool(x)\n","        x = F.relu(self.conv2(x))\n","        x = self.pool(x)\n","        x = x.view(x.size(0), -1)\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","        return x\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, data, labels, word_to_index):\n","        self.data = data\n","        self.labels = labels\n","        self.word_to_index = word_to_index\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        tokens = self.data.iloc[idx]\n","        indices = [self.word_to_index[word] for word in tokens if word in self.word_to_index]\n","        padded_indices = indices + [0] * (MAX_SEQUENCE_LENGTH - len(indices))\n","        return torch.tensor(padded_indices), torch.tensor(self.labels.iloc[idx])\n","\n","# Create Custom Datasets and DataLoaders\n","train_dataset = CustomDataset(series_of_lists_train, df_train['sentiment'], word_to_index)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","\n","val_dataset = CustomDataset(series_of_lists_valid, df_val['sentiment'], word_to_index)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","\n","# Initialize the model\n","vocab_size = len(vocab)\n","embedding_dim = 100\n","num_classes = 3\n","model = CNNModel(vocab_size, embedding_dim, num_classes)\n","\n","# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training loop\n","num_epochs = 10\n","for epoch in range(num_epochs):\n","    model.train()\n","    for inputs, labels in train_loader:\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","    model.eval()\n","    val_loss = 0.0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for inputs, labels in val_loader:\n","            outputs = model(inputs)\n","            val_loss += criterion(outputs, labels).item()\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n","          f\"Validation Loss: {val_loss / len(val_loader):.4f} - \"\n","          f\"Validation Accuracy: {(correct / total) * 100:.2f}%\")\n","\n","# Run the Model on New Data\n","def predict_sentiment(model, text, word_to_index):\n","    tokens = word_tokenize(text)\n","    indices = [word_to_index[word] for word in tokens if word in word_to_index]\n","    padded_indices = indices + [0] * (MAX_SEQUENCE_LENGTH - len(indices))\n","    inputs = torch.tensor(padded_indices).unsqueeze(0)\n","    outputs = model(inputs)\n","    _, predicted = torch.max(outputs, 1)\n","    return predicted.item()\n","\n","new_text = \"getting borderlands\"\n","predicted_label = predict_sentiment(model, new_text, word_to_index)\n","print(f\"Predicted Label: {predicted_label}\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1NFANVPM09ka8M_aqZiqIk6lG84113t9B","timestamp":1691157500526},{"file_id":"1G4XJpCTsOt9olIGDIEA0YCc1Cy2N2jT7","timestamp":1691120327124}],"authorship_tag":"ABX9TyMk9YNZRYn5/9+jVWEgYsL7"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}