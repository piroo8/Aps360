{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"RetkJTy_LCkk","outputId":"f7d225e7-8ec3-47ce-f963-170894cb85dd"},"outputs":[{"name":"stdout","output_type":"stream","text":["CUDA device name: NVIDIA GeForce GTX 1650 Ti with Max-Q Design\n"]}],"source":["import torch\n","\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print(f\"CUDA device name: {torch.cuda.get_device_name()}\")\n","else:\n","    print(\"CUDA is not available.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iYJzPFoQGQoj","outputId":"959817d0-ad50-4055-e899-03e26b1b2ae3","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: emoji in c:\\users\\pierr\\.conda\\envs\\aps360\\lib\\site-packages (2.7.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install emoji --upgrade"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BPtrjX9yGW50","outputId":"96c4d35d-286a-45c9-c416-206bd1589bf1","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: nltk in c:\\users\\pierr\\.conda\\envs\\aps360\\lib\\site-packages (3.8.1)\n","Requirement already satisfied: click in c:\\users\\pierr\\.conda\\envs\\aps360\\lib\\site-packages (from nltk) (8.0.4)\n","Requirement already satisfied: joblib in c:\\users\\pierr\\.conda\\envs\\aps360\\lib\\site-packages (from nltk) (1.3.1)\n","Requirement already satisfied: regex>=2021.8.3 in c:\\users\\pierr\\.conda\\envs\\aps360\\lib\\site-packages (from nltk) (2023.6.3)\n","Requirement already satisfied: tqdm in c:\\users\\pierr\\.conda\\envs\\aps360\\lib\\site-packages (from nltk) (4.65.0)\n","Requirement already satisfied: colorama in c:\\users\\pierr\\.conda\\envs\\aps360\\lib\\site-packages (from click->nltk) (0.4.6)\n"]}],"source":["!pip install nltk"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qbZESrVGGYg7","outputId":"abec65be-c041-4120-ef43-4964f8217f5e","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: contractions in c:\\users\\pierr\\.conda\\envs\\aps360\\lib\\site-packages (0.1.73)\n","Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\pierr\\.conda\\envs\\aps360\\lib\\site-packages (from contractions) (0.0.24)\n","Requirement already satisfied: anyascii in c:\\users\\pierr\\.conda\\envs\\aps360\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n","Requirement already satisfied: pyahocorasick in c:\\users\\pierr\\.conda\\envs\\aps360\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n"]}],"source":["!pip install contractions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IXuI5rbGGZor","outputId":"63c89be9-0477-446d-a4f8-5e9a01f20855","tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     C:\\Users\\pierr\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\pierr\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\pierr\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import pandas as pd\n","import re\n","import emoji\n","\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk.sentiment.util import mark_negation\n","import nltk\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","import contractions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nZiKxYPmGa-j","outputId":"7f703979-9f5c-4189-ec2b-2b3fbdf25bcd","tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\pierr\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["nltk.download('wordnet')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2jBK6ciHGm1e","outputId":"6305e127-164b-41a5-bdc1-66da543f9542","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["     id      company sentiment  \\\n","0  2401  Borderlands  Positive   \n","1  2401  Borderlands  Positive   \n","2  2401  Borderlands  Positive   \n","3  2401  Borderlands  Positive   \n","4  2401  Borderlands  Positive   \n","\n","                                               tweet  \n","0  im getting on borderlands and i will murder yo...  \n","1  I am coming to the borders and I will kill you...  \n","2  im getting on borderlands and i will kill you ...  \n","3  im coming on borderlands and i will murder you...  \n","4  im getting on borderlands 2 and i will murder ...  \n","     id    company   sentiment  \\\n","0  3364   Facebook  Irrelevant   \n","1   352     Amazon     Neutral   \n","2  8312  Microsoft    Negative   \n","3  4371      CS-GO    Negative   \n","4  4433     Google     Neutral   \n","\n","                                               tweet  \n","0  I mentioned on Facebook that I was struggling ...  \n","1  BBC News - Amazon boss Jeff Bezos rejects clai...  \n","2  @Microsoft Why do I pay for WORD when it funct...  \n","3  CSGO matchmaking is so full of closet hacking,...  \n","4  Now the President is slapping Americans in the...  \n"]}],"source":["# Loading the data\n","train_path = \"twitter_training.csv\"\n","val_path = \"twitter_validation.csv\"\n","# Note: ideally data doesnt have column headers but we add them to easy manipulation\n","headers = [\"id\", \"company\", \"sentiment\", \"tweet\"]\n","df_train = pd.read_csv(train_path, names=headers)\n","df_val = pd.read_csv(val_path, names=headers)\n","\n","print(df_train.head())\n","print(df_val.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HX2bpl4AGzhP","tags":[]},"outputs":[],"source":["abbreviations = {\n","    \"$\" : \" dollar \",\n","    \"â‚¬\" : \" euro \",\n","    \"4ao\" : \"for adults only\",\n","    \"a.m\" : \"before midday\",\n","    \"a3\" : \"anytime anywhere anyplace\",\n","    \"aamof\" : \"as a matter of fact\",\n","    \"acct\" : \"account\",\n","    \"adih\" : \"another day in hell\",\n","    \"afaic\" : \"as far as i am concerned\",\n","    \"afaict\" : \"as far as i can tell\",\n","    \"afaik\" : \"as far as i know\",\n","    \"afair\" : \"as far as i remember\",\n","    \"afk\" : \"away from keyboard\",\n","    \"app\" : \"application\",\n","    \"approx\" : \"approximately\",\n","    \"apps\" : \"applications\",\n","    \"asap\" : \"as soon as possible\",\n","    \"asl\" : \"age, sex, location\",\n","    \"atk\" : \"at the keyboard\",\n","    \"ave.\" : \"avenue\",\n","    \"aymm\" : \"are you my mother\",\n","    \"ayor\" : \"at your own risk\",\n","    \"b&b\" : \"bed and breakfast\",\n","    \"b+b\" : \"bed and breakfast\",\n","    \"b.c\" : \"before christ\",\n","    \"b2b\" : \"business to business\",\n","    \"b2c\" : \"business to customer\",\n","    \"b4\" : \"before\",\n","    \"b4n\" : \"bye for now\",\n","    \"b@u\" : \"back at you\",\n","    \"bae\" : \"before anyone else\",\n","    \"bak\" : \"back at keyboard\",\n","    \"bbbg\" : \"bye bye be good\",\n","    \"bbc\" : \"british broadcasting corporation\",\n","    \"bbias\" : \"be back in a second\",\n","    \"bbl\" : \"be back later\",\n","    \"bbs\" : \"be back soon\",\n","    \"be4\" : \"before\",\n","    \"bfn\" : \"bye for now\",\n","    \"blvd\" : \"boulevard\",\n","    \"bout\" : \"about\",\n","    \"brb\" : \"be right back\",\n","    \"bros\" : \"brothers\",\n","    \"brt\" : \"be right there\",\n","    \"bsaaw\" : \"big smile and a wink\",\n","    \"btw\" : \"by the way\",\n","    \"bwl\" : \"bursting with laughter\",\n","    \"c/o\" : \"care of\",\n","    \"cet\" : \"central european time\",\n","    \"cf\" : \"compare\",\n","    \"cia\" : \"central intelligence agency\",\n","    \"csl\" : \"can not stop laughing\",\n","    \"cu\" : \"see you\",\n","    \"cul8r\" : \"see you later\",\n","    \"cv\" : \"curriculum vitae\",\n","    \"cwot\" : \"complete waste of time\",\n","    \"cya\" : \"see you\",\n","    \"cyt\" : \"see you tomorrow\",\n","    \"dae\" : \"does anyone else\",\n","    \"dbmib\" : \"do not bother me i am busy\",\n","    \"diy\" : \"do it yourself\",\n","    \"dm\" : \"direct message\",\n","    \"dwh\" : \"during work hours\",\n","    \"e123\" : \"easy as one two three\",\n","    \"eet\" : \"eastern european time\",\n","    \"eg\" : \"example\",\n","    \"embm\" : \"early morning business meeting\",\n","    \"encl\" : \"enclosed\",\n","    \"encl.\" : \"enclosed\",\n","    \"etc\" : \"and so on\",\n","    \"faq\" : \"frequently asked questions\",\n","    \"fawc\" : \"for anyone who cares\",\n","    \"fb\" : \"facebook\",\n","    \"fc\" : \"fingers crossed\",\n","    \"fig\" : \"figure\",\n","    \"fimh\" : \"forever in my heart\",\n","    \"ft.\" : \"feet\",\n","    \"ft\" : \"featuring\",\n","    \"ftl\" : \"for the loss\",\n","    \"ftw\" : \"for the win\",\n","    \"fwiw\" : \"for what it is worth\",\n","    \"fyi\" : \"for your information\",\n","    \"g9\" : \"genius\",\n","    \"gahoy\" : \"get a hold of yourself\",\n","    \"gal\" : \"get a life\",\n","    \"gcse\" : \"general certificate of secondary education\",\n","    \"gfn\" : \"gone for now\",\n","    \"gg\" : \"good game\",\n","    \"gl\" : \"good luck\",\n","    \"glhf\" : \"good luck have fun\",\n","    \"gmt\" : \"greenwich mean time\",\n","    \"gmta\" : \"great minds think alike\",\n","    \"gn\" : \"good night\",\n","    \"g.o.a.t\" : \"greatest of all time\",\n","    \"goat\" : \"greatest of all time\",\n","    \"goi\" : \"get over it\",\n","    \"gps\" : \"global positioning system\",\n","    \"gr8\" : \"great\",\n","    \"gratz\" : \"congratulations\",\n","    \"gyal\" : \"girl\",\n","    \"h&c\" : \"hot and cold\",\n","    \"hp\" : \"horsepower\",\n","    \"hr\" : \"hour\",\n","    \"hrh\" : \"his royal highness\",\n","    \"ht\" : \"height\",\n","    \"ibrb\" : \"i will be right back\",\n","    \"ic\" : \"i see\",\n","    \"icq\" : \"i seek you\",\n","    \"icymi\" : \"in case you missed it\",\n","    \"idc\" : \"i do not care\",\n","    \"idgadf\" : \"i do not give a damn fuck\",\n","    \"idgaf\" : \"i do not give a fuck\",\n","    \"idk\" : \"i do not know\",\n","    \"ie\" : \"that is\",\n","    \"i.e\" : \"that is\",\n","    \"ifyp\" : \"i feel your pain\",\n","    \"IG\" : \"instagram\",\n","    \"iirc\" : \"if i remember correctly\",\n","    \"ilu\" : \"i love you\",\n","    \"ily\" : \"i love you\",\n","    \"imho\" : \"in my humble opinion\",\n","    \"imo\" : \"in my opinion\",\n","    \"imu\" : \"i miss you\",\n","    \"iow\" : \"in other words\",\n","    \"irl\" : \"in real life\",\n","    \"j4f\" : \"just for fun\",\n","    \"jic\" : \"just in case\",\n","    \"jk\" : \"just kidding\",\n","    \"jsyk\" : \"just so you know\",\n","    \"l8r\" : \"later\",\n","    \"lb\" : \"pound\",\n","    \"lbs\" : \"pounds\",\n","    \"ldr\" : \"long distance relationship\",\n","    \"lmao\" : \"laugh my ass off\",\n","    \"lmfao\" : \"laugh my fucking ass off\",\n","    \"lol\" : \"laughing out loud\",\n","    \"ltd\" : \"limited\",\n","    \"ltns\" : \"long time no see\",\n","    \"m8\" : \"mate\",\n","    \"mf\" : \"motherfucker\",\n","    \"mfs\" : \"motherfuckers\",\n","    \"mfw\" : \"my face when\",\n","    \"mofo\" : \"motherfucker\",\n","    \"mph\" : \"miles per hour\",\n","    \"mr\" : \"mister\",\n","    \"mrw\" : \"my reaction when\",\n","    \"ms\" : \"miss\",\n","    \"mte\" : \"my thoughts exactly\",\n","    \"nagi\" : \"not a good idea\",\n","    \"nbc\" : \"national broadcasting company\",\n","    \"nbd\" : \"not big deal\",\n","    \"nfs\" : \"not for sale\",\n","    \"ngl\" : \"not going to lie\",\n","    \"nhs\" : \"national health service\",\n","    \"nrn\" : \"no reply necessary\",\n","    \"nsfl\" : \"not safe for life\",\n","    \"nsfw\" : \"not safe for work\",\n","    \"nth\" : \"nice to have\",\n","    \"nvr\" : \"never\",\n","    \"nyc\" : \"new york city\",\n","    \"oc\" : \"original content\",\n","    \"og\" : \"original\",\n","    \"ohp\" : \"overhead projector\",\n","    \"oic\" : \"oh i see\",\n","    \"omdb\" : \"over my dead body\",\n","    \"omg\" : \"oh my god\",\n","    \"omw\" : \"on my way\",\n","    \"p.a\" : \"per annum\",\n","    \"p.m\" : \"after midday\",\n","    \"pm\" : \"prime minister\",\n","    \"poc\" : \"people of color\",\n","    \"pov\" : \"point of view\",\n","    \"pp\" : \"pages\",\n","    \"ppl\" : \"people\",\n","    \"prw\" : \"parents are watching\",\n","    \"ps\" : \"postscript\",\n","    \"pt\" : \"point\",\n","    \"ptb\" : \"please text back\",\n","    \"pto\" : \"please turn over\",\n","    \"qpsa\" : \"what happens\", #\"que pasa\",\n","    \"ratchet\" : \"rude\",\n","    \"rbtl\" : \"read between the lines\",\n","    \"rlrt\" : \"real life retweet\",\n","    \"rofl\" : \"rolling on the floor laughing\",\n","    \"roflol\" : \"rolling on the floor laughing out loud\",\n","    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n","    \"rt\" : \"retweet\",\n","    \"ruok\" : \"are you ok\",\n","    \"sfw\" : \"safe for work\",\n","    \"sk8\" : \"skate\",\n","    \"smh\" : \"shake my head\",\n","    \"sq\" : \"square\",\n","    \"srsly\" : \"seriously\",\n","    \"ssdd\" : \"same stuff different day\",\n","    \"tbh\" : \"to be honest\",\n","    \"tbs\" : \"tablespooful\",\n","    \"tbsp\" : \"tablespooful\",\n","    \"tfw\" : \"that feeling when\",\n","    \"thks\" : \"thank you\",\n","    \"tho\" : \"though\",\n","    \"thx\" : \"thank you\",\n","    \"tia\" : \"thanks in advance\",\n","    \"til\" : \"today i learned\",\n","    \"tl;dr\" : \"too long i did not read\",\n","    \"tldr\" : \"too long i did not read\",\n","    \"tmb\" : \"tweet me back\",\n","    \"tntl\" : \"trying not to laugh\",\n","    \"ttyl\" : \"talk to you later\",\n","    \"u\" : \"you\",\n","    \"u2\" : \"you too\",\n","    \"u4e\" : \"yours for ever\",\n","    \"utc\" : \"coordinated universal time\",\n","    \"w/\" : \"with\",\n","    \"w/o\" : \"without\",\n","    \"w8\" : \"wait\",\n","    \"wassup\" : \"what is up\",\n","    \"wb\" : \"welcome back\",\n","    \"wtf\" : \"what the fuck\",\n","    \"wtg\" : \"way to go\",\n","    \"wtpa\" : \"where the party at\",\n","    \"wuf\" : \"where are you from\",\n","    \"wuzup\" : \"what is up\",\n","    \"wywh\" : \"wish you were here\",\n","    \"yd\" : \"yard\",\n","    \"ygtr\" : \"you got that right\",\n","    \"ynk\" : \"you never know\",\n","    \"zzz\" : \"sleeping bored and tired\"\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H9qMwKl5G1Is","tags":[]},"outputs":[],"source":["# Step 1: Remove NaN values\n","def remove_nan_values(df, columns):\n","    for column in columns:\n","        df = df[df[column].notna()]  # Remove rows with NaN values\n","        df = df[df[column].astype(bool)]  # Remove rows with empty lists or lists with only white spaces\n","    return df\n","\n","# Step 2: Assign tweet labels\n","def one_hot_encode_sentiments(sentiment): #make neutral and irrelavent same number, and look over\n","    if sentiment == 'Positive':\n","        return 2\n","    elif sentiment == 'Negative':\n","        return 0\n","    elif sentiment == 'Neutral':\n","        return 1\n","    elif sentiment == 'Irrelevant':\n","        return 1\n","\n","def expand_slang(text):\n","    tokens = word_tokenize(text)\n","    tokens = [abbreviations.get(word.lower(), word) for word in tokens]\n","    text = ' '.join(tokens)\n","    return text\n","\n","\n","# Step 3: Clean tweets\n","def clean_tweets(tweet):\n","    # Remove @mentions\n","    tweet = re.sub(r'@\\w+', '', tweet)\n","\n","    # Remove URLs\n","    tweet = re.sub(r'http\\S+|www\\S+', '', tweet)\n","\n","    # Remove hashtags\n","    tweet = re.sub(r'#\\w+', '', tweet)\n","\n","    # Convert emojis to text\n","    tweet = emoji.demojize(tweet)\n","\n","    # Expand contractions\n","    tweet = contractions.fix(tweet)\n","    ##############################3\n","\n","    # Remove numbers\n","    tweet = re.sub(r'\\d+', '', tweet)\n","\n","    # Remove punctuation\n","    tweet = re.sub(r'[^\\w\\s]', '', tweet)\n","\n","    # Remove excessive whitespace\n","    tweet = re.sub(r'\\s+', ' ', tweet)\n","\n","    # Standardize repeated characters\n","    tweet = re.sub(r'(.)\\1+', r'\\1\\1', tweet)\n","\n","    #Expand slang terms\n","    tweet = expand_slang(tweet)\n","\n","    # Tokenize the tweet\n","    tokens = word_tokenize(tweet)\n","\n","    # Lowercase and normalize\n","    tokens = [token.lower() for token in tokens]\n","\n","    # Remove stop words\n","    stop_words = set(stopwords.words('english'))\n","    tokens = [token for token in tokens if token.lower() not in stop_words]\n","\n","    # Handle abbreviations and acronyms\n","    abbreviation_words = {\"lol\": \"laughing out loud\", \"btw\": \"by the way\"}  # Add more as needed\n","    tokens = [abbreviation_words.get(token.lower(), token) for token in tokens]\n","\n","    # Lemmatization\n","    lemmatizer = WordNetLemmatizer()\n","    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n","\n","    # Handle negation using mark_negation\n","    #tokens = mark_negation(tokens)\n","    #tweet = \"I don't like this product. It is not good.\"\n","    #['I', \"don't\", 'like_NEG', 'this_NEG', 'product_NEG', '.', 'It_NEG', 'is_NEG', 'not_NEG', 'good_NEG', '.']\n","\n","    # Uncomment the following lines to include POS tags as comments\n","    #pos_tags = nltk.pos_tag(tokens)\n","    #tokens_with_pos = [f\"{token}/{pos}\" for token, pos in pos_tags]\n","\n","    return tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o4XLR8RRG7PD","outputId":"b8c5d446-4755-4b56-c25d-bc84b6a3e493","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["     id      company sentiment  \\\n","0  2401  Borderlands  Positive   \n","1  2401  Borderlands  Positive   \n","2  2401  Borderlands  Positive   \n","3  2401  Borderlands  Positive   \n","4  2401  Borderlands  Positive   \n","\n","                                               tweet  \n","0  im getting on borderlands and i will murder yo...  \n","1  I am coming to the borders and I will kill you...  \n","2  im getting on borderlands and i will kill you ...  \n","3  im coming on borderlands and i will murder you...  \n","4  im getting on borderlands 2 and i will murder ...  \n","     id    company   sentiment  \\\n","0  3364   Facebook  Irrelevant   \n","1   352     Amazon     Neutral   \n","2  8312  Microsoft    Negative   \n","3  4371      CS-GO    Negative   \n","4  4433     Google     Neutral   \n","\n","                                               tweet  \n","0  I mentioned on Facebook that I was struggling ...  \n","1  BBC News - Amazon boss Jeff Bezos rejects clai...  \n","2  @Microsoft Why do I pay for WORD when it funct...  \n","3  CSGO matchmaking is so full of closet hacking,...  \n","4  Now the President is slapping Americans in the...  \n"]}],"source":["# Step 1: Remove NaN values\n","columns_to_check = [\"id\", \"company\", \"sentiment\", \"tweet\"]\n","df_train = remove_nan_values(df_train, columns_to_check)\n","df_val = remove_nan_values(df_val, columns_to_check)\n","\n","print(df_train.head())\n","print(df_val.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"obq9v6IMG8i9","outputId":"b506731f-d0c4-40b3-f2a0-72d1af50b701","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["     id      company  sentiment  \\\n","0  2401  Borderlands          2   \n","1  2401  Borderlands          2   \n","2  2401  Borderlands          2   \n","3  2401  Borderlands          2   \n","4  2401  Borderlands          2   \n","\n","                                               tweet  \n","0  im getting on borderlands and i will murder yo...  \n","1  I am coming to the borders and I will kill you...  \n","2  im getting on borderlands and i will kill you ...  \n","3  im coming on borderlands and i will murder you...  \n","4  im getting on borderlands 2 and i will murder ...  \n","     id    company  sentiment  \\\n","0  3364   Facebook          1   \n","1   352     Amazon          1   \n","2  8312  Microsoft          0   \n","3  4371      CS-GO          0   \n","4  4433     Google          1   \n","\n","                                               tweet  \n","0  I mentioned on Facebook that I was struggling ...  \n","1  BBC News - Amazon boss Jeff Bezos rejects clai...  \n","2  @Microsoft Why do I pay for WORD when it funct...  \n","3  CSGO matchmaking is so full of closet hacking,...  \n","4  Now the President is slapping Americans in the...  \n"]}],"source":["# Step 2: Assign tweet labels and remove rows with 'Irrelevant' sentiment\n","df_train['sentiment'] = df_train['sentiment'].apply(one_hot_encode_sentiments)\n","df_val['sentiment'] = df_val['sentiment'].apply(one_hot_encode_sentiments)\n","\n","# Remove rows with 'Irrelevant' sentiment from the DataFrame\n","df_train = df_train.dropna(subset=['sentiment'])\n","df_val = df_val.dropna(subset=['sentiment'])\n","\n","print(df_train.head())\n","print(df_val.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W_5aw4nVG91C","outputId":"2ca290d8-bb4f-4d02-c442-37161d80059a","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["     id      company  sentiment                          tweet\n","0  2401  Borderlands          2  [getting, borderland, murder]\n","1  2401  Borderlands          2         [coming, border, kill]\n","2  2401  Borderlands          2    [getting, borderland, kill]\n","3  2401  Borderlands          2   [coming, borderland, murder]\n","4  2401  Borderlands          2  [getting, borderland, murder]\n","     id    company  sentiment  \\\n","0  3364   Facebook          1   \n","1   352     Amazon          1   \n","2  8312  Microsoft          0   \n","3  4371      CS-GO          0   \n","4  4433     Google          1   \n","\n","                                               tweet  \n","0  [mentioned, facebook, struggling, motivation, ...  \n","1  [british, broadcasting, corporation, news, ama...  \n","2  [pay, word, function, poorly, chromebook, face...  \n","3  [csgo, matchmaking, full, closet, hacking, tru...  \n","4  [president, slapping, american, face, really, ...  \n"]}],"source":["# Step 3: Clean tweets\n","df_train['tweet'] = df_train['tweet'].apply(clean_tweets)\n","df_val['tweet'] = df_val['tweet'].apply(clean_tweets)\n","\n","print(df_train.head())\n","print(df_val.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bv96Y-HCG_XA","outputId":"35e509aa-b9ca-4564-df38-e50110045ef1","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["     id      company  sentiment                          tweet\n","0  2401  Borderlands          2  [getting, borderland, murder]\n","1  2401  Borderlands          2         [coming, border, kill]\n","2  2401  Borderlands          2    [getting, borderland, kill]\n","3  2401  Borderlands          2   [coming, borderland, murder]\n","4  2401  Borderlands          2  [getting, borderland, murder]\n","      id               company  sentiment  \\\n","0   3364              Facebook          1   \n","1    352                Amazon          1   \n","4   4433                Google          1   \n","6   7925             MaddenNFL          2   \n","7  11332  TomClancysRainbowSix          2   \n","\n","                                               tweet  \n","0  [mentioned, facebook, struggling, motivation, ...  \n","1  [british, broadcasting, corporation, news, ama...  \n","4  [president, slapping, american, face, really, ...  \n","6  [thank, new, te, austin, hooper, orange, brown...  \n","7  [rocket, league, sea, thief, rainbow, six, sie...  \n"]}],"source":["# Step 1: Remove NaN values\n","columns_to_check = [\"id\", \"company\", \"sentiment\", \"tweet\"]\n","df_train = remove_nan_values(df_train, columns_to_check)\n","df_val = remove_nan_values(df_val, columns_to_check)\n","\n","print(df_train.head())\n","print(df_val.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zD3zM14xHAlZ","outputId":"4ed14f8a-1a09-4f58-c950-5818dab4fd0a","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["     id      company  sentiment                          tweet\n","0  2401  Borderlands          2  [getting, borderland, murder]\n","1  2401  Borderlands          2         [coming, border, kill]\n","2  2401  Borderlands          2    [getting, borderland, kill]\n","3  2401  Borderlands          2   [coming, borderland, murder]\n","4  2401  Borderlands          2  [getting, borderland, murder]\n","      id               company  sentiment  \\\n","0   3364              Facebook          1   \n","1    352                Amazon          1   \n","4   4433                Google          1   \n","6   7925             MaddenNFL          2   \n","7  11332  TomClancysRainbowSix          2   \n","\n","                                               tweet  \n","0  [mentioned, facebook, struggling, motivation, ...  \n","1  [british, broadcasting, corporation, news, ama...  \n","4  [president, slapping, american, face, really, ...  \n","6  [thank, new, te, austin, hooper, orange, brown...  \n","7  [rocket, league, sea, thief, rainbow, six, sie...  \n"]}],"source":["# Print the resulting DataFrame\n","print(df_train.head())\n","print(df_val.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OqMQesJiHCzv","tags":[]},"outputs":[],"source":["# Assuming your DataFrame is named df\n","import torch\n","# Drop the 'id' column\n","df_train_new = df_train.drop('id', axis=1)\n","df_val_new = df_val.drop('id', axis=1)\n","\n","# Export the DataFrame to a CSV file\n","df_train_new.to_csv('df_train_new_int.csv', index=False)\n","df_val_new.to_csv('df_val_new_int.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NojZ5_e5HEhs","outputId":"cf705846-355f-4b58-d1b3-6264afa074e3","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["       company  sentiment                          tweet\n","0  Borderlands          2  [getting, borderland, murder]\n","1  Borderlands          2         [coming, border, kill]\n","2  Borderlands          2    [getting, borderland, kill]\n","3  Borderlands          2   [coming, borderland, murder]\n","4  Borderlands          2  [getting, borderland, murder]\n"]}],"source":["print(df_train_new[:5])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h5Gww7ThyEd8","tags":[],"outputId":"5696f323-6f33-4555-f097-ac80efc6426d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train dataset shape: torch.Size([50380, 100, 100])\n","Valid dataset shape: torch.Size([734, 100, 100])\n"]}],"source":["import torch\n","import pandas as pd\n","from collections import Counter\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","# Load GloVe embeddings\n","def load_glove_embeddings(embeddings_path):\n","    embeddings_index = {}\n","    with open(embeddings_path, \"r\", encoding=\"utf-8\") as f:\n","        for line in f:\n","            values = line.split()\n","            word = values[0]\n","            coefs = torch.tensor([float(val) for val in values[1:]])\n","            embeddings_index[word] = coefs\n","    return embeddings_index\n","\n","def get_vocab_dict(word_counts):\n","    sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n","    vocab_dict = {word: idx for idx, (word, _) in enumerate(sorted_words)}\n","    return vocab_dict\n","\n","def indices_to_embeddings(indices, embeddings_index, embedding_dim, vocab_dict):\n","    embeddings = []\n","    for idx in indices:\n","        word = list(vocab_dict.keys())[list(vocab_dict.values()).index(idx)]\n","        if word in embeddings_index:\n","            embeddings.append(embeddings_index[word])\n","        else:\n","            embeddings.append(torch.zeros(embedding_dim))\n","    return embeddings\n","\n","# Paths\n","glove_path = \"glove.twitter.27B.100d.txt\"  # Replace with your actual path\n","data_path_train = \"df_train_new_int.csv\"  # Replace with your actual path\n","data_path_valid = \"df_val_new_int.csv\"  # Replace with your actual path\n","\n","# Load data\n","df_train_new = pd.read_csv(data_path_train)\n","df_val_new = pd.read_csv(data_path_valid)\n","\n","# Flatten the list of lists into a single list\n","all_words = [word for sublist in df_train_new['tweet'] for word in sublist]\n","all_words_valid = [word for sublist in df_val_new['tweet'] for word in sublist]\n","\n","# Create a vocabulary dictionary with words ordered by frequency\n","word_counts = Counter(all_words + all_words_valid)\n","vocab_dict = get_vocab_dict(word_counts)\n","\n","# Convert words to indices for each tweet\n","sequences_of_indices_train = [[vocab_dict[word] for word in sublist] for sublist in df_train_new['tweet']]\n","sequences_of_indices_valid = [[vocab_dict[word] for word in sublist] for sublist in df_val_new['tweet']]\n","\n","# Load GloVe embeddings\n","glove_embeddings = load_glove_embeddings(glove_path)\n","\n","# Convert indices to GloVe embeddings\n","embedding_dim = 100  # Assuming you're using 100-dimensional GloVe embeddings\n","\n","train_embeddings = [indices_to_embeddings(seq, glove_embeddings, embedding_dim, vocab_dict) for seq in sequences_of_indices_train]\n","valid_embeddings = [indices_to_embeddings(seq, glove_embeddings, embedding_dim, vocab_dict) for seq in sequences_of_indices_valid]\n","\n","# Ensure train_embeddings and valid_embeddings are lists of lists of tensors\n","train_embeddings = [torch.stack(embedding_list) for embedding_list in train_embeddings]\n","valid_embeddings = [torch.stack(embedding_list) for embedding_list in valid_embeddings]\n","\n","max_train_seq_length = 100\n","\n","# Pad sequences and create tensors\n","padded_train_embeddings = pad_sequence(\n","    [torch.cat((seq, torch.zeros(max_train_seq_length - len(seq), embedding_dim))) if len(seq) < max_train_seq_length else seq[:max_train_seq_length] for seq in train_embeddings],\n","    batch_first=True, padding_value=0.0\n",")\n","\n","padded_valid_embeddings = pad_sequence(\n","    [torch.cat((seq, torch.zeros(max_train_seq_length - len(seq), embedding_dim))) if len(seq) < max_train_seq_length else seq[:max_train_seq_length] for seq in valid_embeddings],\n","    batch_first=True, padding_value=0.0\n",")\n","\n","# Convert labels to tensors\n","train_labels = torch.tensor(df_train_new['sentiment'].values, dtype=torch.long)\n","valid_labels = torch.tensor(df_val_new['sentiment'].values, dtype=torch.long)\n","\n","# Create datasets and data loaders\n","batch_size = 32\n","train_dataset = TensorDataset(padded_train_embeddings, train_labels)\n","valid_dataset = TensorDataset(padded_valid_embeddings, valid_labels)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n","\n","# Print shapes for verification\n","print(\"Train dataset shape:\", padded_train_embeddings.shape)\n","print(\"Valid dataset shape:\", padded_valid_embeddings.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8TYhhS-oJC_4","tags":[]},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.metrics import confusion_matrix\n","import numpy as np\n","\n","class SentimentCNNLSTM(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, num_classes, dropout_prob=0.0):\n","        super(SentimentCNNLSTM, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","\n","        self.conv_layers = nn.Sequential(\n","            nn.Conv1d(in_channels=embedding_dim, out_channels=128, kernel_size=3),\n","            nn.ReLU(),\n","            nn.MaxPool1d(kernel_size=2),\n","            nn.Conv1d(in_channels=128, out_channels=64, kernel_size=3),\n","            nn.ReLU(),\n","            nn.MaxPool1d(kernel_size=2)\n","        )\n","\n","        self.lstm = nn.LSTM(input_size=64, hidden_size=hidden_size, num_layers=num_layers,\n","                            batch_first=True, bidirectional=True, dropout=dropout_prob)\n","\n","        self.fc = nn.Linear(hidden_size * 2, num_classes)\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x.long())  # Ensure input tensor is of type Long\n","        conv_out = self.conv_layers(embedded.permute(0, 2, 1))\n","        lstm_out, _ = self.lstm(conv_out.permute(0, 2, 1))\n","        lstm_out = lstm_out[:, -1, :]\n","        logits = self.fc(lstm_out)\n","        return logits\n","\n","def train(model, train_dataloader, learning_rate, epochs, optimizer):\n","    model.train()\n","\n","    loss_fn = nn.CrossEntropyLoss()\n","\n","    for epoch in range(epochs):\n","        running_loss = 0.0\n","        correct_predictions = 0\n","        total_predictions = 0\n","\n","        for batch in train_dataloader:\n","            inputs = batch[0]\n","            labels = batch[1]\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = loss_fn(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","\n","            _, predicted = torch.max(outputs, 1)\n","            total_predictions += labels.size(0)\n","            correct_predictions += (predicted == labels).sum().item()\n","\n","        epoch_loss = running_loss / len(train_dataloader)\n","        training_accuracy = correct_predictions / total_predictions\n","        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {training_accuracy * 100:.2f}%\")\n","\n","    return training_accuracy\n","\n","def evaluate(model, dataloader):\n","    model.eval()  # Set the model to evaluation mode\n","    all_labels = []\n","    all_predictions = []\n","\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            inputs = batch[0]\n","            labels = batch[1]\n","\n","            outputs = model(inputs)\n","            _, predicted = torch.max(outputs, 1)\n","\n","            all_labels.extend(labels.numpy())\n","            all_predictions.extend(predicted.numpy())\n","\n","    confusion = confusion_matrix(all_labels, all_predictions)\n","    return confusion"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":246},"id":"iCQKFBuFJVJn","outputId":"68d64051-1127-410e-d10e-2c5322a85736","tags":[],"executionInfo":{"status":"error","timestamp":1691116496390,"user_tz":240,"elapsed":14,"user":{"displayName":"Pierre Ishak","userId":"08507075842828312856"}}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-771807f11a37>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Define hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0membedding_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnum_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'vocab_dict' is not defined"]}],"source":["# Define hyperparameters\n","vocab_size = len(vocab_dict)\n","embedding_dim = 100\n","hidden_size = 128\n","num_layers = 3\n","num_classes = 3  # Assuming you have 3 sentiment classes (Positive, Neutral, Negative)\n","\n","# Create the model\n","cnn_lstm_model = SentimentCNNLSTM(vocab_size, embedding_dim, hidden_size, num_layers, num_classes, dropout_prob=0.5)\n","\n","# Define loss function and optimizer\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(cnn_lstm_model.parameters(), lr=0.001)\n","\n","# Train the model\n","train(cnn_lstm_model, train_loader, learning_rate=0.001, epochs=5, optimizer=optimizer)\n","\n","# Evaluate the model\n","confusion_matrix = evaluate(cnn_lstm_model, valid_loader)\n","print(\"Confusion Matrix:\")\n","print(confusion_matrix)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Qb_II8-JzIM"},"outputs":[],"source":["pip install hyperopt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1tB7hZnhJ0JO"},"outputs":[],"source":["from hyperopt import hp, fmin, tpe, Trials\n","\n","# Define the hyperparameter search space\n","space = {\n","    'lr': hp.choice('lr', [1e-5, 1e-4, 1e-3, 1e-2, 5e-5, 5e-4]),\n","    'hidden_size': hp.choice('hidden_size', [32, 64, 128, 256]),\n","    'num_layers': hp.choice('num_layers', [1, 2, 3]),\n","    'dropout': hp.choice('dropout', [0.0, 0.2,0.35, 0.5, 0.75,0.8,1]),  # Dropout rate\n","    'optimizer': hp.choice('optimizer', ['adam', 'sgd', 'rmsprop']),  # Optimizer choice\n","    'epochs': 2  # Run each model for 1 epoch\n","}\n","\n","# Initialize a list to store the top 10 hyperparameter configurations and their accuracies\n","top_configs = []\n","\n","# Define the objective function to optimize (in this case, the negative accuracy)\n","def objective(params):\n","    lr = params['lr']\n","    hidden_size = params['hidden_size']\n","    num_layers = params['num_layers']\n","    dropout = params['dropout']\n","    optimizer_choice = params['optimizer']\n","    epochs = params['epochs']\n","\n","    # Create the model with the given hyperparameters\n","    model = SentimentRNN(198, hidden_size, num_layers, dropout)\n","\n","    # Choose optimizer\n","    optimizer = None\n","    if optimizer_choice == 'adam':\n","        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    elif optimizer_choice == 'sgd':\n","        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n","    elif optimizer_choice == 'rmsprop':\n","        optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n","\n","    # Train the model and get the accuracy\n","    accuracy = train(model, train_loader, lr, epochs, optimizer)\n","\n","    # Add the hyperparameters and accuracy to the top_configs list\n","    top_configs.append({'lr': lr, 'hidden_size': hidden_size, 'num_layers': num_layers,\n","                        'dropout': dropout, 'optimizer': optimizer_choice,\n","                        'accuracy': accuracy})\n","\n","    # Since hyperopt minimizes the objective function, we need to return the negative accuracy\n","    return -accuracy\n","\n","# Initialize the Trials object to track the optimization process\n","trials = Trials()\n","\n","# Run the hyperparameter search using the Tree of Parzen Estimators (TPE) algorithm\n","best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=100, trials=trials)\n","\n","# Sort the top_configs list based on accuracy in descending order\n","top_configs.sort(key=lambda x: x['accuracy'], reverse=True)\n","\n","# Print the 10 best hyperparameter configurations and their accuracies\n","for i, config in enumerate(top_configs[:10]):\n","    print(f\"Top {i+1} Hyperparameters:\")\n","    print(f\"Learning Rate: {config['lr']}\")\n","    print(f\"Hidden Layer Size: {config['hidden_size']}\")\n","    print(f\"Number of Hidden Layers: {config['num_layers']}\")\n","    print(f\"Dropout Rate: {config['dropout']}\")\n","    print(f\"Optimizer: {config['optimizer']}\")\n","    print(f\"Accuracy: {config['accuracy']}\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WXSo7Ke6J4no"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","train_path = \"sentiment-emotion-labelled_Dell_tweets.csv\"\n","# use pathlib to load path\n","df = pd.read_csv(train_path, error_bad_lines=False)\n","\n","fig = plt.figure(figsize=(8,5))\n","ax = sns.barplot(x=df.sentiment.unique(),y=df.sentiment.value_counts());\n","ax.set(xlabel='Labels')\n","# Note: The label distribution is not even!"]}],"metadata":{"colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.17"}},"nbformat":4,"nbformat_minor":0}